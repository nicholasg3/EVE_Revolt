{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Plan Jan 07\n",
    "### Look at evolution of topics over time (for leavers vs. remainers)\n",
    "1. Identify topics (open-coding): LDA, PCA, network of terms. Also read comments / threads and apply my own labels.\n",
    "2. Create \"dictionaries\" to map sets of terms to (potentially) theoretically interesting topics / codes.\n",
    "3. Do this once overall, and next for over-lapping time-windows to see evolution of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk.sentiment.vader\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"csm subforums all posts cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>Subforum</th>\n",
       "      <th>subsectionpage</th>\n",
       "      <th>all_thread_ids</th>\n",
       "      <th>Thread_Title</th>\n",
       "      <th>Thread_Page</th>\n",
       "      <th>Date_of_post</th>\n",
       "      <th>Username</th>\n",
       "      <th>Post</th>\n",
       "      <th>...</th>\n",
       "      <th>Comma</th>\n",
       "      <th>Colon</th>\n",
       "      <th>SemiC</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Dash</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>Parenth</th>\n",
       "      <th>OtherP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>1498941</td>\n",
       "      <td>[csm6] may summit topics thread index</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-04-19</td>\n",
       "      <td>Killer2</td>\n",
       "      <td>so we don't lose sight of all the summit topic...</td>\n",
       "      <td>...</td>\n",
       "      <td>3,61</td>\n",
       "      <td>1,81</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,60</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,30</td>\n",
       "      <td>1,20</td>\n",
       "      <td>3,61</td>\n",
       "      <td>0,90</td>\n",
       "      <td>0,30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>1498941</td>\n",
       "      <td>[csm6] may summit topics thread index</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-04-21</td>\n",
       "      <td>Shepard Book</td>\n",
       "      <td>what happened to the post of the long list of ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>4,17</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>1498941</td>\n",
       "      <td>[csm6] may summit topics thread index</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-04-22</td>\n",
       "      <td>Olivia Ironsides</td>\n",
       "      <td>csm5 was bad, fixing minor things/voting in th...</td>\n",
       "      <td>...</td>\n",
       "      <td>4,55</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>9,09</td>\n",
       "      <td>4,55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>1498941</td>\n",
       "      <td>[csm6] may summit topics thread index</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-04-25</td>\n",
       "      <td>Consortium Agent</td>\n",
       "      <td>csm6 opted to forego player base consideration...</td>\n",
       "      <td>...</td>\n",
       "      <td>0,00</td>\n",
       "      <td>1,27</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>2,53</td>\n",
       "      <td>2,53</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>1498941</td>\n",
       "      <td>[csm6] may summit topics thread index</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-04-25</td>\n",
       "      <td>Killer2</td>\n",
       "      <td>actually, you'll find that the topics we plan ...</td>\n",
       "      <td>...</td>\n",
       "      <td>2,33</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>2,33</td>\n",
       "      <td>0,00</td>\n",
       "      <td>2,33</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>1498941</td>\n",
       "      <td>[csm6] may summit topics thread index</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-04-26</td>\n",
       "      <td>Falin Whalen</td>\n",
       "      <td>i sence much... hostility, in this one.prithee...</td>\n",
       "      <td>...</td>\n",
       "      <td>5,71</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>2,86</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>1498941</td>\n",
       "      <td>[csm6] may summit topics thread index</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-04-29</td>\n",
       "      <td>Consortium Agent</td>\n",
       "      <td>odd, because i just listened to the fireside c...</td>\n",
       "      <td>...</td>\n",
       "      <td>2,50</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>1,25</td>\n",
       "      <td>0,00</td>\n",
       "      <td>8,75</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>1498941</td>\n",
       "      <td>[csm6] may summit topics thread index</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-04-29</td>\n",
       "      <td>Consortium Agent</td>\n",
       "      <td>i'm only hostile when the csm says one thing b...</td>\n",
       "      <td>...</td>\n",
       "      <td>1,30</td>\n",
       "      <td>1,30</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>6,49</td>\n",
       "      <td>2,60</td>\n",
       "      <td>12,99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>1498941</td>\n",
       "      <td>[csm6] may summit topics thread index</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-05-01</td>\n",
       "      <td>Mr DurkaDur</td>\n",
       "      <td>your totally right, 6 of the 7 summit topics \"...</td>\n",
       "      <td>...</td>\n",
       "      <td>18,75</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>37,50</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>1498941</td>\n",
       "      <td>[csm6] may summit topics thread index</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-05-21</td>\n",
       "      <td>Hirana Yoshida</td>\n",
       "      <td>oy! will we get a preliminary report before re...</td>\n",
       "      <td>...</td>\n",
       "      <td>6,03</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,86</td>\n",
       "      <td>4,31</td>\n",
       "      <td>6,03</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>1,72</td>\n",
       "      <td>4,31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>1498941</td>\n",
       "      <td>[csm6] may summit topics thread index</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-06-15</td>\n",
       "      <td>Salomei</td>\n",
       "      <td>that's not bias, it's an inescapable result. n...</td>\n",
       "      <td>...</td>\n",
       "      <td>4,17</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>12,50</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>1498941</td>\n",
       "      <td>[csm6] may summit topics thread index</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-07-13</td>\n",
       "      <td>Jon Helldrunk</td>\n",
       "      <td>yes, exactly...what happened to the post of th...</td>\n",
       "      <td>...</td>\n",
       "      <td>3,85</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>3,85</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>1178661</td>\n",
       "      <td>welcome to the assembly hall</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-09-14</td>\n",
       "      <td>mazzilliu</td>\n",
       "      <td>in this forum, any eve player can raise an iss...</td>\n",
       "      <td>...</td>\n",
       "      <td>3,96</td>\n",
       "      <td>0,54</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,54</td>\n",
       "      <td>0,18</td>\n",
       "      <td>1,26</td>\n",
       "      <td>0,72</td>\n",
       "      <td>0,90</td>\n",
       "      <td>0,72</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>1178661</td>\n",
       "      <td>welcome to the assembly hall</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-09-14</td>\n",
       "      <td>De'Veldrin</td>\n",
       "      <td>very nicely done mazzilliu. are the mods going...</td>\n",
       "      <td>...</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>7,69</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>1178661</td>\n",
       "      <td>welcome to the assembly hall</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-09-14</td>\n",
       "      <td>mazzilliu</td>\n",
       "      <td>continued from the op------------eve wiki link...</td>\n",
       "      <td>...</td>\n",
       "      <td>4,96</td>\n",
       "      <td>2,48</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,83</td>\n",
       "      <td>40,50</td>\n",
       "      <td>3,31</td>\n",
       "      <td>2,48</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>1178661</td>\n",
       "      <td>welcome to the assembly hall</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-09-14</td>\n",
       "      <td>Treelox</td>\n",
       "      <td>shame that those who most need to read the fir...</td>\n",
       "      <td>...</td>\n",
       "      <td>5,56</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>1178661</td>\n",
       "      <td>welcome to the assembly hall</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-09-16</td>\n",
       "      <td>CCP Diagoras</td>\n",
       "      <td>bluebars</td>\n",
       "      <td>...</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>1178661</td>\n",
       "      <td>welcome to the assembly hall</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-09-16</td>\n",
       "      <td>mazzilliu</td>\n",
       "      <td>now i'm bona-fide important!</td>\n",
       "      <td>...</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>20,00</td>\n",
       "      <td>20,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>20,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>1178661</td>\n",
       "      <td>welcome to the assembly hall</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-09-16</td>\n",
       "      <td>Lori Carlyle</td>\n",
       "      <td>yours is missing at least one item, ankhesenta...</td>\n",
       "      <td>...</td>\n",
       "      <td>3,70</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>3,70</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>11,11</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>1178661</td>\n",
       "      <td>welcome to the assembly hall</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-09-16</td>\n",
       "      <td>mazzilliu</td>\n",
       "      <td>say what?</td>\n",
       "      <td>...</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>50,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>1178661</td>\n",
       "      <td>welcome to the assembly hall</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-09-16</td>\n",
       "      <td>Lori Carlyle</td>\n",
       "      <td>now why would i want to do that? i'm not csm a...</td>\n",
       "      <td>...</td>\n",
       "      <td>5,13</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>1,28</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>7,69</td>\n",
       "      <td>5,13</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>1178661</td>\n",
       "      <td>welcome to the assembly hall</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-09-16</td>\n",
       "      <td>mazzilliu</td>\n",
       "      <td>well if you won't tell me what it is, then :ef...</td>\n",
       "      <td>...</td>\n",
       "      <td>9,09</td>\n",
       "      <td>18,18</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>9,09</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>1178661</td>\n",
       "      <td>welcome to the assembly hall</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-09-16</td>\n",
       "      <td>Lori Carlyle</td>\n",
       "      <td>yeah, effort for a computer game is little off...</td>\n",
       "      <td>...</td>\n",
       "      <td>9,09</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>9,09</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>1178661</td>\n",
       "      <td>welcome to the assembly hall</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-09-16</td>\n",
       "      <td>Emily Spankratchet</td>\n",
       "      <td>hey! irl i'm paid to update a wiki and i don't...</td>\n",
       "      <td>...</td>\n",
       "      <td>5,26</td>\n",
       "      <td>2,63</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>7,89</td>\n",
       "      <td>5,26</td>\n",
       "      <td>10,53</td>\n",
       "      <td>13,16</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>1178661</td>\n",
       "      <td>welcome to the assembly hall</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-09-17</td>\n",
       "      <td>Lori Carlyle</td>\n",
       "      <td>to be fair, there is nothing missing just a fe...</td>\n",
       "      <td>...</td>\n",
       "      <td>4,94</td>\n",
       "      <td>1,23</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,62</td>\n",
       "      <td>0,00</td>\n",
       "      <td>2,47</td>\n",
       "      <td>2,47</td>\n",
       "      <td>0,62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>1178661</td>\n",
       "      <td>welcome to the assembly hall</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-09-17</td>\n",
       "      <td>Emily Spankratchet</td>\n",
       "      <td>good points, well-made. there's a difference b...</td>\n",
       "      <td>...</td>\n",
       "      <td>5,20</td>\n",
       "      <td>1,16</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,58</td>\n",
       "      <td>0,00</td>\n",
       "      <td>1,16</td>\n",
       "      <td>5,78</td>\n",
       "      <td>5,20</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>1178661</td>\n",
       "      <td>welcome to the assembly hall</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-09-17</td>\n",
       "      <td>mazzilliu</td>\n",
       "      <td>put up or shut up. i have no idea what things ...</td>\n",
       "      <td>...</td>\n",
       "      <td>3,23</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>4,84</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>1178661</td>\n",
       "      <td>welcome to the assembly hall</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-09-17</td>\n",
       "      <td>Lori Carlyle</td>\n",
       "      <td>emily, oh it is very usefull, &amp; and many thank...</td>\n",
       "      <td>...</td>\n",
       "      <td>4,32</td>\n",
       "      <td>1,44</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,72</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,72</td>\n",
       "      <td>1,44</td>\n",
       "      <td>0,72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>1178661</td>\n",
       "      <td>welcome to the assembly hall</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-09-17</td>\n",
       "      <td>mazzilliu</td>\n",
       "      <td>it's not my business to go and edit past csm's...</td>\n",
       "      <td>...</td>\n",
       "      <td>2,38</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>4,76</td>\n",
       "      <td>4,76</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>1178661</td>\n",
       "      <td>welcome to the assembly hall</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-09-17</td>\n",
       "      <td>Lori Carlyle</td>\n",
       "      <td>i know the answers is no, but as csm can you p...</td>\n",
       "      <td>...</td>\n",
       "      <td>7,69</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>3,85</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  ID       Subforum  subsectionpage  all_thread_ids  \\\n",
       "0            1   1  Assembly Hall               1         1498941   \n",
       "1            2   2  Assembly Hall               1         1498941   \n",
       "2            3   3  Assembly Hall               1         1498941   \n",
       "3            4   4  Assembly Hall               1         1498941   \n",
       "4            5   5  Assembly Hall               1         1498941   \n",
       "5            6   6  Assembly Hall               1         1498941   \n",
       "6            7   7  Assembly Hall               1         1498941   \n",
       "7            8   8  Assembly Hall               1         1498941   \n",
       "8            9   9  Assembly Hall               1         1498941   \n",
       "9           10  10  Assembly Hall               1         1498941   \n",
       "10          11  11  Assembly Hall               1         1498941   \n",
       "11          12  12  Assembly Hall               1         1498941   \n",
       "12          13  13  Assembly Hall               1         1178661   \n",
       "13          14  14  Assembly Hall               1         1178661   \n",
       "14          15  15  Assembly Hall               1         1178661   \n",
       "15          16  16  Assembly Hall               1         1178661   \n",
       "16          17  17  Assembly Hall               1         1178661   \n",
       "17          18  18  Assembly Hall               1         1178661   \n",
       "18          19  19  Assembly Hall               1         1178661   \n",
       "19          20  20  Assembly Hall               1         1178661   \n",
       "20          21  21  Assembly Hall               1         1178661   \n",
       "21          22  22  Assembly Hall               1         1178661   \n",
       "22          23  23  Assembly Hall               1         1178661   \n",
       "23          24  24  Assembly Hall               1         1178661   \n",
       "24          25  25  Assembly Hall               1         1178661   \n",
       "25          26  26  Assembly Hall               1         1178661   \n",
       "26          27  27  Assembly Hall               1         1178661   \n",
       "27          28  28  Assembly Hall               1         1178661   \n",
       "28          29  29  Assembly Hall               1         1178661   \n",
       "29          30  30  Assembly Hall               1         1178661   \n",
       "\n",
       "                             Thread_Title  Thread_Page Date_of_post  \\\n",
       "0   [csm6] may summit topics thread index            1   2011-04-19   \n",
       "1   [csm6] may summit topics thread index            1   2011-04-21   \n",
       "2   [csm6] may summit topics thread index            1   2011-04-22   \n",
       "3   [csm6] may summit topics thread index            1   2011-04-25   \n",
       "4   [csm6] may summit topics thread index            1   2011-04-25   \n",
       "5   [csm6] may summit topics thread index            1   2011-04-26   \n",
       "6   [csm6] may summit topics thread index            1   2011-04-29   \n",
       "7   [csm6] may summit topics thread index            1   2011-04-29   \n",
       "8   [csm6] may summit topics thread index            1   2011-05-01   \n",
       "9   [csm6] may summit topics thread index            1   2011-05-21   \n",
       "10  [csm6] may summit topics thread index            1   2011-06-15   \n",
       "11  [csm6] may summit topics thread index            1   2011-07-13   \n",
       "12           welcome to the assembly hall            1   2009-09-14   \n",
       "13           welcome to the assembly hall            1   2009-09-14   \n",
       "14           welcome to the assembly hall            1   2009-09-14   \n",
       "15           welcome to the assembly hall            1   2009-09-14   \n",
       "16           welcome to the assembly hall            1   2009-09-16   \n",
       "17           welcome to the assembly hall            1   2009-09-16   \n",
       "18           welcome to the assembly hall            1   2009-09-16   \n",
       "19           welcome to the assembly hall            1   2009-09-16   \n",
       "20           welcome to the assembly hall            1   2009-09-16   \n",
       "21           welcome to the assembly hall            1   2009-09-16   \n",
       "22           welcome to the assembly hall            1   2009-09-16   \n",
       "23           welcome to the assembly hall            1   2009-09-16   \n",
       "24           welcome to the assembly hall            1   2009-09-17   \n",
       "25           welcome to the assembly hall            1   2009-09-17   \n",
       "26           welcome to the assembly hall            1   2009-09-17   \n",
       "27           welcome to the assembly hall            1   2009-09-17   \n",
       "28           welcome to the assembly hall            1   2009-09-17   \n",
       "29           welcome to the assembly hall            1   2009-09-17   \n",
       "\n",
       "              Username                                               Post  \\\n",
       "0              Killer2  so we don't lose sight of all the summit topic...   \n",
       "1         Shepard Book  what happened to the post of the long list of ...   \n",
       "2     Olivia Ironsides  csm5 was bad, fixing minor things/voting in th...   \n",
       "3     Consortium Agent  csm6 opted to forego player base consideration...   \n",
       "4              Killer2  actually, you'll find that the topics we plan ...   \n",
       "5         Falin Whalen  i sence much... hostility, in this one.prithee...   \n",
       "6     Consortium Agent  odd, because i just listened to the fireside c...   \n",
       "7     Consortium Agent  i'm only hostile when the csm says one thing b...   \n",
       "8          Mr DurkaDur  your totally right, 6 of the 7 summit topics \"...   \n",
       "9       Hirana Yoshida  oy! will we get a preliminary report before re...   \n",
       "10             Salomei  that's not bias, it's an inescapable result. n...   \n",
       "11       Jon Helldrunk  yes, exactly...what happened to the post of th...   \n",
       "12           mazzilliu  in this forum, any eve player can raise an iss...   \n",
       "13          De'Veldrin  very nicely done mazzilliu. are the mods going...   \n",
       "14           mazzilliu  continued from the op------------eve wiki link...   \n",
       "15             Treelox  shame that those who most need to read the fir...   \n",
       "16        CCP Diagoras                                           bluebars   \n",
       "17           mazzilliu                       now i'm bona-fide important!   \n",
       "18        Lori Carlyle  yours is missing at least one item, ankhesenta...   \n",
       "19           mazzilliu                                          say what?   \n",
       "20        Lori Carlyle  now why would i want to do that? i'm not csm a...   \n",
       "21           mazzilliu  well if you won't tell me what it is, then :ef...   \n",
       "22        Lori Carlyle  yeah, effort for a computer game is little off...   \n",
       "23  Emily Spankratchet  hey! irl i'm paid to update a wiki and i don't...   \n",
       "24        Lori Carlyle  to be fair, there is nothing missing just a fe...   \n",
       "25  Emily Spankratchet  good points, well-made. there's a difference b...   \n",
       "26           mazzilliu  put up or shut up. i have no idea what things ...   \n",
       "27        Lori Carlyle  emily, oh it is very usefull, & and many thank...   \n",
       "28           mazzilliu  it's not my business to go and edit past csm's...   \n",
       "29        Lori Carlyle  i know the answers is no, but as csm can you p...   \n",
       "\n",
       "    ...    Comma  Colon SemiC  QMark Exclam   Dash  Quote Apostro Parenth  \\\n",
       "0   ...     3,61   1,81  0,00   0,60   0,00   0,30   1,20    3,61    0,90   \n",
       "1   ...     0,00   0,00  0,00   4,17   0,00   0,00   0,00    0,00    0,00   \n",
       "2   ...     4,55   0,00  0,00   0,00   0,00   0,00   0,00    0,00    9,09   \n",
       "3   ...     0,00   1,27  0,00   0,00   0,00   0,00   0,00    2,53    2,53   \n",
       "4   ...     2,33   0,00  0,00   0,00   0,00   2,33   0,00    2,33    0,00   \n",
       "5   ...     5,71   0,00  0,00   2,86   0,00   0,00   0,00    0,00    0,00   \n",
       "6   ...     2,50   0,00  0,00   0,00   0,00   1,25   0,00    8,75    0,00   \n",
       "7   ...     1,30   1,30  0,00   0,00   0,00   0,00   0,00    6,49    2,60   \n",
       "8   ...    18,75   0,00  0,00   0,00   0,00   0,00  37,50    0,00    0,00   \n",
       "9   ...     6,03   0,00  0,00   0,86   4,31   6,03   0,00    0,00    1,72   \n",
       "10  ...     4,17   0,00  0,00   0,00   0,00   0,00   0,00   12,50    0,00   \n",
       "11  ...     3,85   0,00  0,00   3,85   0,00   0,00   0,00    0,00    0,00   \n",
       "12  ...     3,96   0,54  0,00   0,54   0,18   1,26   0,72    0,90    0,72   \n",
       "13  ...     0,00   0,00  0,00   7,69   0,00   0,00   0,00    0,00    0,00   \n",
       "14  ...     4,96   2,48  0,00   0,00   0,83  40,50   3,31    2,48    0,00   \n",
       "15  ...     5,56   0,00  0,00   0,00   0,00   0,00   0,00    0,00    0,00   \n",
       "16  ...     0,00   0,00  0,00   0,00   0,00   0,00   0,00    0,00    0,00   \n",
       "17  ...     0,00   0,00  0,00   0,00  20,00  20,00   0,00   20,00    0,00   \n",
       "18  ...     3,70   0,00  0,00   3,70   0,00   0,00   0,00   11,11    0,00   \n",
       "19  ...     0,00   0,00  0,00  50,00   0,00   0,00   0,00    0,00    0,00   \n",
       "20  ...     5,13   0,00  0,00   1,28   0,00   0,00   0,00    7,69    5,13   \n",
       "21  ...     9,09  18,18  0,00   0,00   0,00   0,00   0,00    9,09    0,00   \n",
       "22  ...     9,09   0,00  0,00   0,00   0,00   0,00   0,00    9,09    0,00   \n",
       "23  ...     5,26   2,63  0,00   0,00   7,89   5,26  10,53   13,16    0,00   \n",
       "24  ...     4,94   1,23  0,00   0,00   0,00   0,62   0,00    2,47    2,47   \n",
       "25  ...     5,20   1,16  0,00   0,58   0,00   1,16   5,78    5,20    0,00   \n",
       "26  ...     3,23   0,00  0,00   0,00   0,00   0,00   0,00    4,84    0,00   \n",
       "27  ...     4,32   1,44  0,00   0,00   0,00   0,72   0,00    0,72    1,44   \n",
       "28  ...     2,38   0,00  0,00   0,00   0,00   0,00   4,76    4,76    0,00   \n",
       "29  ...     7,69   0,00  0,00   3,85   0,00   0,00   0,00    0,00    0,00   \n",
       "\n",
       "   OtherP  \n",
       "0    0,30  \n",
       "1    0,00  \n",
       "2    4,55  \n",
       "3    0,00  \n",
       "4    0,00  \n",
       "5    0,00  \n",
       "6    0,00  \n",
       "7   12,99  \n",
       "8    0,00  \n",
       "9    4,31  \n",
       "10   0,00  \n",
       "11   0,00  \n",
       "12   0,00  \n",
       "13   0,00  \n",
       "14   0,00  \n",
       "15   0,00  \n",
       "16   0,00  \n",
       "17   0,00  \n",
       "18   0,00  \n",
       "19   0,00  \n",
       "20   0,00  \n",
       "21   0,00  \n",
       "22   0,00  \n",
       "23   0,00  \n",
       "24   0,62  \n",
       "25   0,58  \n",
       "26   0,00  \n",
       "27   0,72  \n",
       "28   0,00  \n",
       "29   0,00  \n",
       "\n",
       "[30 rows x 103 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# posts = dataset['Post']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select posts during the incarna crisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subset posts during the incarna crisis\n",
    "# dataset['Date_of_post'] =  pd.to_datetime(dataset['Date_of_post'],infer_datetime_format=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crisis_start = datetime.date(2011,3,1)\n",
    "# crisis_end = datetime.date(2011,9,1)\n",
    "# # (dataset['Date_of_post'].head() > crisis_start) & (dataset['Date_of_post'].head() < crisis_end)\n",
    "# mask = (crisis_start <= dataset['Date_of_post']) & (dataset['Date_of_post'] <= crisis_end)\n",
    "# posts_crisis = posts.loc[mask]\n",
    "# print('posts during crisis = ' + str( len(posts_crisis)))\n",
    "# posts_crisis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Do topic analysis for threads, not posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>Subforum</th>\n",
       "      <th>subsectionpage</th>\n",
       "      <th>all_thread_ids</th>\n",
       "      <th>Thread_Title</th>\n",
       "      <th>Thread_Page</th>\n",
       "      <th>Date_of_post</th>\n",
       "      <th>Username</th>\n",
       "      <th>Post</th>\n",
       "      <th>...</th>\n",
       "      <th>Comma</th>\n",
       "      <th>Colon</th>\n",
       "      <th>SemiC</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Dash</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>Parenth</th>\n",
       "      <th>OtherP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>250172</td>\n",
       "      <td>262350</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>73</td>\n",
       "      <td>6339</td>\n",
       "      <td>hot topics of the assembly hall - aug 23 - 0.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-09-06</td>\n",
       "      <td>ccp spitfire</td>\n",
       "      <td>originally authored by obsidian hawk.ok so we ...</td>\n",
       "      <td>...</td>\n",
       "      <td>4,14</td>\n",
       "      <td>5,80</td>\n",
       "      <td>0,00</td>\n",
       "      <td>1,10</td>\n",
       "      <td>0,83</td>\n",
       "      <td>2,21</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,55</td>\n",
       "      <td>0,00</td>\n",
       "      <td>2,21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>250173</td>\n",
       "      <td>262351</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>73</td>\n",
       "      <td>6339</td>\n",
       "      <td>hot topics of the assembly hall - aug 23 - 0.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-09-06</td>\n",
       "      <td>ccp spitfire</td>\n",
       "      <td>blasters 2.0blasters need a buffrailguns balan...</td>\n",
       "      <td>...</td>\n",
       "      <td>1,00</td>\n",
       "      <td>0,50</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>1,99</td>\n",
       "      <td>3,98</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,50</td>\n",
       "      <td>0,00</td>\n",
       "      <td>5,97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>250174</td>\n",
       "      <td>262352</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>73</td>\n",
       "      <td>6339</td>\n",
       "      <td>hot topics of the assembly hall - aug 23 - 0.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-09-14</td>\n",
       "      <td>cyllan anassan</td>\n",
       "      <td>any chance of making cosmos missions expired t...</td>\n",
       "      <td>...</td>\n",
       "      <td>5,41</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>250175</td>\n",
       "      <td>262353</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>73</td>\n",
       "      <td>6339</td>\n",
       "      <td>hot topics of the assembly hall - aug 23 - 0.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-09-16</td>\n",
       "      <td>samillian</td>\n",
       "      <td>bumpthis should be a sticky.</td>\n",
       "      <td>...</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>250176</td>\n",
       "      <td>262354</td>\n",
       "      <td>Assembly Hall</td>\n",
       "      <td>73</td>\n",
       "      <td>6339</td>\n",
       "      <td>hot topics of the assembly hall - aug 23 - 0.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-09-16</td>\n",
       "      <td>hirana yoshida</td>\n",
       "      <td>it was a sticky until someone pressed the wron...</td>\n",
       "      <td>...</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>5,88</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      ID       Subforum  subsectionpage  all_thread_ids  \\\n",
       "0      250172  262350  Assembly Hall              73            6339   \n",
       "1      250173  262351  Assembly Hall              73            6339   \n",
       "2      250174  262352  Assembly Hall              73            6339   \n",
       "3      250175  262353  Assembly Hall              73            6339   \n",
       "4      250176  262354  Assembly Hall              73            6339   \n",
       "\n",
       "                                        Thread_Title  Thread_Page  \\\n",
       "0  hot topics of the assembly hall - aug 23 - 0.0...            1   \n",
       "1  hot topics of the assembly hall - aug 23 - 0.0...            1   \n",
       "2  hot topics of the assembly hall - aug 23 - 0.0...            1   \n",
       "3  hot topics of the assembly hall - aug 23 - 0.0...            1   \n",
       "4  hot topics of the assembly hall - aug 23 - 0.0...            1   \n",
       "\n",
       "  Date_of_post        Username  \\\n",
       "0   2011-09-06    ccp spitfire   \n",
       "1   2011-09-06    ccp spitfire   \n",
       "2   2011-09-14  cyllan anassan   \n",
       "3   2011-09-16       samillian   \n",
       "4   2011-09-16  hirana yoshida   \n",
       "\n",
       "                                                Post  ...    Comma Colon  \\\n",
       "0  originally authored by obsidian hawk.ok so we ...  ...     4,14  5,80   \n",
       "1  blasters 2.0blasters need a buffrailguns balan...  ...     1,00  0,50   \n",
       "2  any chance of making cosmos missions expired t...  ...     5,41  0,00   \n",
       "3                       bumpthis should be a sticky.  ...     0,00  0,00   \n",
       "4  it was a sticky until someone pressed the wron...  ...     0,00  0,00   \n",
       "\n",
       "  SemiC QMark Exclam  Dash Quote Apostro Parenth OtherP  \n",
       "0  0,00  1,10   0,83  2,21  0,00    0,55    0,00   2,21  \n",
       "1  0,00  0,00   1,99  3,98  0,00    0,50    0,00   5,97  \n",
       "2  0,00  0,00   0,00  0,00  0,00    0,00    0,00   0,00  \n",
       "3  0,00  0,00   0,00  0,00  0,00    0,00    0,00   0,00  \n",
       "4  0,00  5,88   0,00  0,00  0,00    0,00    0,00   0,00  \n",
       "\n",
       "[5 rows x 103 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sorted = dataset.sort_values(by=['all_thread_ids','Date_of_post']).reset_index(drop=True)\n",
    "# dataset_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get all posts from each thread and concatenate them (Sorting by time)\n",
    "# include columns for first post date, last post date, median post date\n",
    "all_thread_ids = dataset_sorted['all_thread_ids'].unique()\n",
    "\n",
    "thread_dict = {}\n",
    "for thread_id in all_thread_ids:\n",
    "    thread_items = dataset_sorted.loc[dataset_sorted[\"all_thread_ids\"]==6339]\n",
    "    first_date = min(thread_items[\"Date_of_post\"])\n",
    "    last_date =  max(thread_items[\"Date_of_post\"])\n",
    "    median_date = thread_items[\"Date_of_post\"][round(len(thread_items)/2)]\n",
    "    thread_posts = thread_items['Post'].str.cat(sep =\" ---------------- \")\n",
    "    thread_title = thread_items['Thread_Title'][0]\n",
    "    first_poster = thread_items['Username'][0]\n",
    "    most_freq_poster = thread_items['Username'].value_counts().idxmax()\n",
    "    thread_dict[thread_id] = {'first_date':first_date,'last_date':last_date,'median_date':median_date,'thread_posts':thread_posts, 'thread_title':thread_title, 'first_poster':first_poster, 'most_freq_poster':most_freq_poster} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = pd.DataFrame.from_dict(thread_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember: Change Tokenization to Phrases after proof-of-concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Just checking what the tokenization looks like\n",
    "\n",
    "# #tokens = nltk.word_tokenize(posts[0])\n",
    "# post0 = posts_crisis[0]\n",
    "# post0_sent = nltk.sentiment.vader.SentiText(posts_crisis[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# post0_sent.words_and_emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # writing the posts to an external file to be called by the VADER sentiment analyzer?\n",
    "\n",
    "# file = open(\"posts_crisis.txt\",\"w\")\n",
    "# file.write(posts[0])\n",
    "# file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def print_sentiment_scores(sentence):\n",
    "#     snt = analyser.polarity_scores(sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test out the sentiment analyzer on the first 100 posts\n",
    "# y = posts_crisis[0:100].apply(analyser.polarity_scores)\n",
    "# y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# N = len(posts_crisis)\n",
    "# compound = [None for i in range(N)]\n",
    "# negative = [None for i in range(N)]\n",
    "# neutral = [None for i in range(N)]\n",
    "# positive = [None for i in range(N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Very slow. Roughly 20 mins to process 318k posts @ 16k posts/min\n",
    "# start = datetime.datetime.now()\n",
    "# i = 0\n",
    "# for post in posts_crisis: # note, can't index by i\n",
    "    \n",
    "#     try:\n",
    "#         scores = analyser.polarity_scores(post)\n",
    "#         compound[i] = scores[\"compound\"]\n",
    "#         negative[i] = scores[\"neg\"]\n",
    "#         neutral[i] = scores[\"neu\"]\n",
    "#         positive[i] = scores[\"pos\"]\n",
    "#     except:\n",
    "#         print(\"exception at: \"+str(i))\n",
    "#     if i%1000 == 0:\n",
    "#         print(str(i)+\" posts   ||   time = \"+str(datetime.datetime.now()-start))\n",
    "#     i = i + 1\n",
    "# print(\"Sentiment Analysis - Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment_scores = pd.DataFrame({'compound':compound,'negative':negative,'neutral':neutral,'positive':positive})\n",
    "# sentiment_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# len(sentiment_scores) == len(posts_crisis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset_sent = dataset.loc[mask].join(sentiment_scores)\n",
    "# dataset_sent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset_sent = dataset_sent.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # dataset_sent.to_csv('posts_during_incarna_crisis_with_sentiment_scores.csv')\n",
    "# dataset_sent = pd.read_csv('posts_during_incarna_crisis_with_sentiment_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(dataset_sent)\n",
    "# len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aside: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset2 = dataset_sent.groupby([pd.Grouper(key='Date_of_post', freq='W-MON')])['negative','positive'].mean().reset_index().sort_values('Date_of_post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# window = 2\n",
    "\n",
    "# ds2 = dataset2.dropna()\n",
    "# ds3 = ds2.loc[:,ds2.columns!=\"Date_of_post\"].rolling(window).mean().dropna()\n",
    "# ds3 = ds3.join(ds2[\"Date_of_post\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams['figure.figsize'] = [10, 5]\n",
    "# # plt.subplots(2, 2, sharex='col')\n",
    "\n",
    "# title_txt = 'Sentiment Analysis of EVE Online Forumns'\n",
    "\n",
    "# ds3.plot.line(x='Date_of_post',y=['positive','negative'], style=['g-','r-'])\n",
    "# plt.title(title_txt)\n",
    "# plt.ylabel('Sentiment Score \\n('+str(window)+'-week averages)')\n",
    "# plt.legend(loc=3)\n",
    "# plt.grid()\n",
    "\n",
    "\n",
    "# cname = \"net_positive\"\n",
    "# ds4 = ds3\n",
    "# ds4[cname] = ds3['positive']-ds3['negative']\n",
    "# ds4.plot.line(x='Date_of_post',y=cname)\n",
    "# plt.grid()\n",
    "# plt.title(title_txt)\n",
    "# plt.ylabel('Positive minus Negative sentiment scores \\n('+ str(window) + '-week averages)')\n",
    "# plt.legend(loc=3)\n",
    "\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select comments that match the date and rate above some threshold for negativity on sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dsneg = dataset_sent.sort_values(\"negative\",ascending=False)\n",
    "# for post in dsneg[\"Post\"].head():\n",
    "#     print(post)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary:\n",
    "Looking at this post at least, there does seem to be some negative sentiment or at least somewhat raised emotion. Let's move forward with the topic analysis.\n",
    "\n",
    "Why not just start with topic analysis (or Nomological Network Analysis) and see which topics are associated with negativity? Maybe we will re-discover the mechanics behind the Sentiment analyzer, but maybe we will see some interesting stuff too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nicholas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
    "# didn't have gensim pre-loaded so went to terminal and installed it with $pip install gensim\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to perform lemmatize and stem preprocessing steps on the data set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    # using a try block here because there are some cases where this fails (I guess for empty posts)\n",
    "    try: \n",
    "        for token in gensim.utils.simple_preprocess(text):\n",
    "            if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "                result.append(lemmatize_stemming(token))\n",
    "    except:\n",
    "        pass\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a document to preview after preprocessing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    so we don't lose sight of all the summit topic...\n",
       "1    what happened to the post of the long list of ...\n",
       "2    csm5 was bad, fixing minor things/voting in th...\n",
       "3    csm6 opted to forego player base consideration...\n",
       "4    actually, you'll find that the topics we plan ...\n",
       "Name: Post, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = dataset['Post']\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "so we don't lose sight of all the summit topic threads, here's an index with a short summary about each one.team bff & the thousand little things.:in this thread, we welcome the community to offer suggestions about how csm can help team bff focus their efforts to get even more \"bang for buck\". we are not just looking for specific little things that annoy you and that you'd like fixed, but also suggestions about improving the process of identifying and prioritizing them.ship balance:in this thread, we're looking for everything relating to ship balancing, encompassing everything from individual ships to how they interact in fleets. the sky is the limit.nullsec industrialization (\"farms and fields\"):in this thread, we're looking for advice and feedback on how to improve nullsec industrialization. making nullsec less dependent on jita for everything, providing farms and fields for small-gang targets beyond sov structures. this may degenerate into a 'for gods sakes implement the missing dominion features' discussion.customer support:in this thread, we're looking for feedback on ways to improve eve customer support as a whole. it should focus on policies and procedures, whether new or existing, rather than individual petition details.new player experience:in this thread, we'll be discussing the eve npe. ccp has made great strides in improving the overall npe. we would like to gather feedback from current players on ways to improve the npe and ideas to increase player retention (by giving a good npe)ui improvements:in this thread, we're looking for methods to improve the eve ui. feel free to include links to illustrations of your ideas if you're a photoshop wizard.eve marketing:ccp zinfandel will be talking to the csm for an hour about ccp's marketing efforts related to eve. what advice would you want us to give him about seducing unsuspecting gamers into paying for the privilege of being podded? and what honey could he set out to lure back the elusive bittervet?\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['lose', 'sight', 'summit', 'topic', 'thread', 'index', 'short', 'summari', 'team', 'thousand', 'littl', 'thing', 'thread', 'welcom', 'communiti', 'offer', 'suggest', 'help', 'team', 'focus', 'effort', 'bang', 'buck', 'look', 'specif', 'littl', 'thing', 'annoy', 'like', 'fix', 'suggest', 'improv', 'process', 'identifi', 'priorit', 'ship', 'balanc', 'thread', 'look', 'relat', 'ship', 'balanc', 'encompass', 'individu', 'ship', 'interact', 'fleet', 'limit', 'nullsec', 'farm', 'field', 'thread', 'look', 'advic', 'feedback', 'improv', 'nullsec', 'make', 'nullsec', 'depend', 'jita', 'provid', 'farm', 'field', 'small', 'gang', 'target', 'structur', 'degener', 'god', 'sake', 'implement', 'miss', 'dominion', 'featur', 'discuss', 'custom', 'support', 'thread', 'look', 'feedback', 'way', 'improv', 'custom', 'support', 'focus', 'polici', 'procedur', 'exist', 'individu', 'petit', 'detail', 'player', 'experi', 'thread', 'discuss', 'great', 'stride', 'improv', 'overal', 'like', 'gather', 'feedback', 'current', 'player', 'way', 'improv', 'idea', 'increas', 'player', 'retent', 'give', 'good', 'improv', 'thread', 'look', 'method', 'improv', 'feel', 'free', 'includ', 'link', 'illustr', 'idea', 'photoshop', 'wizard', 'market', 'zinfandel', 'talk', 'hour', 'market', 'effort', 'relat', 'advic', 'want', 'seduc', 'unsuspect', 'gamer', 'pay', 'privileg', 'pod', 'honey', 'lure', 'elus', 'bittervet']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[0]\n",
    "print('original document: ')\n",
    "print(doc_sample)\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "# print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### commentary: \n",
    "We might not what to use stemming since it obscures what is being talked about and leaves out local-specific language (like 'afk' and 'ccp' and 'csm'). But for now we can live with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the posts, saving the results as ‘processed_docs’   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lose',\n",
       " 'sight',\n",
       " 'summit',\n",
       " 'topic',\n",
       " 'thread',\n",
       " 'index',\n",
       " 'short',\n",
       " 'summari',\n",
       " 'team',\n",
       " 'thousand',\n",
       " 'littl',\n",
       " 'thing',\n",
       " 'thread',\n",
       " 'welcom',\n",
       " 'communiti',\n",
       " 'offer',\n",
       " 'suggest',\n",
       " 'help',\n",
       " 'team',\n",
       " 'focus',\n",
       " 'effort',\n",
       " 'bang',\n",
       " 'buck',\n",
       " 'look',\n",
       " 'specif',\n",
       " 'littl',\n",
       " 'thing',\n",
       " 'annoy',\n",
       " 'like',\n",
       " 'fix',\n",
       " 'suggest',\n",
       " 'improv',\n",
       " 'process',\n",
       " 'identifi',\n",
       " 'priorit',\n",
       " 'ship',\n",
       " 'balanc',\n",
       " 'thread',\n",
       " 'look',\n",
       " 'relat',\n",
       " 'ship',\n",
       " 'balanc',\n",
       " 'encompass',\n",
       " 'individu',\n",
       " 'ship',\n",
       " 'interact',\n",
       " 'fleet',\n",
       " 'limit',\n",
       " 'nullsec',\n",
       " 'farm',\n",
       " 'field',\n",
       " 'thread',\n",
       " 'look',\n",
       " 'advic',\n",
       " 'feedback',\n",
       " 'improv',\n",
       " 'nullsec',\n",
       " 'make',\n",
       " 'nullsec',\n",
       " 'depend',\n",
       " 'jita',\n",
       " 'provid',\n",
       " 'farm',\n",
       " 'field',\n",
       " 'small',\n",
       " 'gang',\n",
       " 'target',\n",
       " 'structur',\n",
       " 'degener',\n",
       " 'god',\n",
       " 'sake',\n",
       " 'implement',\n",
       " 'miss',\n",
       " 'dominion',\n",
       " 'featur',\n",
       " 'discuss',\n",
       " 'custom',\n",
       " 'support',\n",
       " 'thread',\n",
       " 'look',\n",
       " 'feedback',\n",
       " 'way',\n",
       " 'improv',\n",
       " 'custom',\n",
       " 'support',\n",
       " 'focus',\n",
       " 'polici',\n",
       " 'procedur',\n",
       " 'exist',\n",
       " 'individu',\n",
       " 'petit',\n",
       " 'detail',\n",
       " 'player',\n",
       " 'experi',\n",
       " 'thread',\n",
       " 'discuss',\n",
       " 'great',\n",
       " 'stride',\n",
       " 'improv',\n",
       " 'overal',\n",
       " 'like',\n",
       " 'gather',\n",
       " 'feedback',\n",
       " 'current',\n",
       " 'player',\n",
       " 'way',\n",
       " 'improv',\n",
       " 'idea',\n",
       " 'increas',\n",
       " 'player',\n",
       " 'retent',\n",
       " 'give',\n",
       " 'good',\n",
       " 'improv',\n",
       " 'thread',\n",
       " 'look',\n",
       " 'method',\n",
       " 'improv',\n",
       " 'feel',\n",
       " 'free',\n",
       " 'includ',\n",
       " 'link',\n",
       " 'illustr',\n",
       " 'idea',\n",
       " 'photoshop',\n",
       " 'wizard',\n",
       " 'market',\n",
       " 'zinfandel',\n",
       " 'talk',\n",
       " 'hour',\n",
       " 'market',\n",
       " 'effort',\n",
       " 'relat',\n",
       " 'advic',\n",
       " 'want',\n",
       " 'seduc',\n",
       " 'unsuspect',\n",
       " 'gamer',\n",
       " 'pay',\n",
       " 'privileg',\n",
       " 'pod',\n",
       " 'honey',\n",
       " 'lure',\n",
       " 'elus',\n",
       " 'bittervet']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents.map(preprocess)\n",
    "processed_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may try to add bigrams at a later point. too slow now.\n",
    "# import re\n",
    "# l=0\n",
    "# # add bigrams and trigrams to the dictionary.\n",
    "# for i in range(len(processed_docs)):\n",
    "#     unigrams = processed_docs[i]\n",
    "#     bigrams = nltk.bigrams(unigrams)\n",
    "#     for bigram in bigrams:\n",
    "#         processed_docs[i].append(\" \".join(bigram))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bag of Words on the Data set\n",
    "Create a dictionary from ‘processed_docs’ containing the number of times a word appears in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72505 battlecrab\n",
      "61647 minz\n",
      "31397 vaction\n",
      "33317 vekarra\n",
      "67032 wookie\n",
      "9617 degrad\n",
      "55021 unbl\n",
      "16120 circul\n",
      "72220 ienergi\n",
      "90940 deil\n",
      "94324 tigerit\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter out tokens that appear in\n",
    "- less than 50 posts (absolute number) or\n",
    "- more than 50% of documents (fraction of total corpus size, not absolute number).\n",
    "- after the above two steps, keep only the first 100,000 most frequent tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old_dictionary = dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old len: 116323\n",
      "new len: 6413\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"old len: \"+ str(len(dictionary)))\n",
    "dictionary.filter_extremes(no_below=50, no_above=0.50, keep_n=100000)\n",
    "print(\"new len: \"+ str(len(dictionary)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to ‘bow_corpus’ (bow = bag of words), then check our selected document earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "# bow_corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at the bag of words representation of the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bow_doc_0 = bow_corpus[0]\n",
    "# for i in range(len(bow_doc_0)):\n",
    "#     print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_0[i][0], \n",
    "#                                                dictionary[bow_doc_0[i][0]], \n",
    "# bow_doc_0[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tf-idf model object using models.TfidfModel on ‘bow_corpus’ and save it to ‘tfidf’, then apply transformation to the entire corpus and call it ‘corpus_tfidf’. Finally we preview TF-IDF scores for our first document.\n",
    "\n",
    "TFIDF normalizes the counts of a word in each document by the word's relative rarity in the corpus. Therefore, common words are given lower wieght, and rare words are given more weight in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.17404072393243772),\n",
      " (1, 0.065969816087878136),\n",
      " (2, 0.098403959147352865),\n",
      " (3, 0.10026394901522014),\n",
      " (4, 0.11005797551042822),\n",
      " (5, 0.099034496230123989),\n",
      " (6, 0.056698060606799223),\n",
      " (7, 0.0418135070909888),\n",
      " (8, 0.13400773195241139),\n",
      " (9, 0.118235407374384),\n",
      " (10, 0.065215311947906945),\n",
      " (11, 0.06919979016853374),\n",
      " (12, 0.09876103711097324),\n",
      " (13, 0.081207615246209286),\n",
      " (14, 0.11811343459057873),\n",
      " (15, 0.11500626350574748),\n",
      " (16, 0.052924863402011864),\n",
      " (17, 0.053952513026636582),\n",
      " (18, 0.15079993026441824),\n",
      " (19, 0.056414071689049584),\n",
      " (20, 0.21181091027573931),\n",
      " (21, 0.048199841383928958),\n",
      " (22, 0.1276347516947508),\n",
      " (23, 0.053425457384577095),\n",
      " (24, 0.047857364617419594),\n",
      " (25, 0.12198233200071013),\n",
      " (26, 0.053220786122736716),\n",
      " (27, 0.090794759664156371),\n",
      " (28, 0.056651552799433326),\n",
      " (29, 0.07688459423730995),\n",
      " (30, 0.043313159352001043),\n",
      " (31, 0.1011473487460975),\n",
      " (32, 0.033108702838659805),\n",
      " (33, 0.047962673794692393),\n",
      " (34, 0.046529024002448106),\n",
      " (35, 0.12217357018440367),\n",
      " (36, 0.054560983171000138),\n",
      " (37, 0.063285530059296477),\n",
      " (38, 0.083429797559189486),\n",
      " (39, 0.094009479147786248),\n",
      " (40, 0.054504190474775061),\n",
      " (41, 0.37616692924885475),\n",
      " (42, 0.057619950802704122),\n",
      " (43, 0.050055167662997536),\n",
      " (44, 0.098457689879132781),\n",
      " (45, 0.12912617103256538),\n",
      " (46, 0.065889706989276958),\n",
      " (47, 0.065777379454047907),\n",
      " (48, 0.047098711390260763),\n",
      " (49, 0.052115036906380939),\n",
      " (50, 0.057732937200847337),\n",
      " (51, 0.093256916578659393),\n",
      " (52, 0.19310202115020425),\n",
      " (53, 0.048344348057997998),\n",
      " (54, 0.10787985824352558),\n",
      " (55, 0.037291995727514543),\n",
      " (56, 0.10479585737541475),\n",
      " (57, 0.070018960848234502),\n",
      " (58, 0.060981202577597669),\n",
      " (59, 0.18307328472408813),\n",
      " (60, 0.063915103915035812),\n",
      " (61, 0.071261579726995594),\n",
      " (62, 0.059199399575054004),\n",
      " (63, 0.071138413677309775),\n",
      " (64, 0.12242816794552379),\n",
      " (65, 0.092935285695672359),\n",
      " (66, 0.077913634160339099),\n",
      " (67, 0.078694109246946781),\n",
      " (68, 0.094322328683291809),\n",
      " (69, 0.098552203900126312),\n",
      " (70, 0.095267088289170465),\n",
      " (71, 0.060842447353342197),\n",
      " (72, 0.054176347960896855),\n",
      " (73, 0.11487069391188601),\n",
      " (74, 0.096898494623179288),\n",
      " (75, 0.08016346242048325),\n",
      " (76, 0.088472335847403605),\n",
      " (77, 0.060272217587740845),\n",
      " (78, 0.089867448962226287),\n",
      " (79, 0.046587641525392562),\n",
      " (80, 0.055367627944717057),\n",
      " (81, 0.064120901224047899),\n",
      " (82, 0.094775751871232883),\n",
      " (83, 0.087104468127971221),\n",
      " (84, 0.078659263869348822),\n",
      " (85, 0.056916215503498027),\n",
      " (86, 0.050906132251291571),\n",
      " (87, 0.049776021586667302),\n",
      " (88, 0.13245467096521391),\n",
      " (89, 0.064786679396656693),\n",
      " (90, 0.07308565603063967),\n",
      " (91, 0.2839418467078933),\n",
      " (92, 0.055065174133936211),\n",
      " (93, 0.11985200200563631),\n",
      " (94, 0.030218797522173595),\n",
      " (95, 0.12248512524668628),\n",
      " (96, 0.073474101182931045),\n",
      " (97, 0.11016542692904917)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running LDA using Bag of Words\n",
    "Train our lda model using gensim.models.LdaMulticore and save it to ‘lda_model’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/gensim/models/ldamodel.py:1023: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    }
   ],
   "source": [
    "n_topics = 100\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=n_topics, id2word=dictionary, passes=2, workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the number of topics that were chosen above. Using more topics can discover more fine-grained and interesting themes, but also has a chance at discovering words that seemed to clump together just due to noise.\n",
    "\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.188*\"thank\" + 0.123*\"hope\" + 0.049*\"chairman\" + 0.046*\"comment\" + 0.041*\"address\" + 0.039*\"that\" + 0.019*\"fault\" + 0.018*\"recommend\" + 0.014*\"fund\" + 0.012*\"virtual\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.151*\"goon\" + 0.051*\"mitten\" + 0.025*\"social\" + 0.023*\"trust\" + 0.019*\"announc\" + 0.019*\"individu\" + 0.016*\"think\" + 0.016*\"conflict\" + 0.016*\"feedback\" + 0.015*\"resourc\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.125*\"item\" + 0.055*\"folk\" + 0.055*\"store\" + 0.047*\"pain\" + 0.046*\"dead\" + 0.037*\"model\" + 0.029*\"document\" + 0.023*\"opportun\" + 0.022*\"flip\" + 0.022*\"wear\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.107*\"sorri\" + 0.057*\"abus\" + 0.046*\"unit\" + 0.036*\"laugh\" + 0.031*\"somebodi\" + 0.028*\"realiti\" + 0.028*\"straight\" + 0.025*\"fals\" + 0.022*\"harass\" + 0.021*\"door\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.105*\"develop\" + 0.083*\"voic\" + 0.065*\"process\" + 0.057*\"wasn\" + 0.051*\"cool\" + 0.036*\"code\" + 0.028*\"influenc\" + 0.020*\"zero\" + 0.019*\"approach\" + 0.018*\"work\"\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.098*\"warp\" + 0.062*\"rare\" + 0.056*\"silent\" + 0.046*\"easi\" + 0.028*\"rock\" + 0.027*\"fast\" + 0.023*\"land\" + 0.023*\"align\" + 0.021*\"time\" + 0.021*\"slow\"\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.104*\"kill\" + 0.095*\"bounti\" + 0.086*\"step\" + 0.057*\"sandbox\" + 0.052*\"parti\" + 0.038*\"hunt\" + 0.036*\"right\" + 0.034*\"accept\" + 0.031*\"dude\" + 0.031*\"collect\"\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.158*\"interact\" + 0.083*\"logist\" + 0.060*\"rep\" + 0.051*\"remot\" + 0.040*\"secret\" + 0.039*\"transpar\" + 0.029*\"orca\" + 0.028*\"aggressor\" + 0.027*\"switch\" + 0.027*\"worthless\"\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.138*\"need\" + 0.073*\"seat\" + 0.057*\"ship\" + 0.054*\"buff\" + 0.054*\"boost\" + 0.048*\"freighter\" + 0.040*\"hold\" + 0.040*\"rig\" + 0.037*\"cargo\" + 0.034*\"carri\"\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.183*\"faction\" + 0.096*\"alt\" + 0.066*\"warfar\" + 0.053*\"stand\" + 0.029*\"overview\" + 0.027*\"interfac\" + 0.026*\"grind\" + 0.024*\"info\" + 0.022*\"background\" + 0.018*\"hang\"\n",
      "\n",
      "Topic: 10 \n",
      "Words: 0.117*\"custom\" + 0.097*\"ban\" + 0.070*\"destroy\" + 0.049*\"advantag\" + 0.040*\"unfair\" + 0.035*\"job\" + 0.029*\"paint\" + 0.023*\"unsub\" + 0.021*\"explod\" + 0.019*\"shop\"\n",
      "\n",
      "Topic: 11 \n",
      "Words: 0.141*\"tank\" + 0.124*\"spam\" + 0.106*\"shield\" + 0.054*\"booster\" + 0.043*\"armor\" + 0.031*\"resist\" + 0.020*\"stop\" + 0.017*\"neut\" + 0.016*\"buffer\" + 0.014*\"hull\"\n",
      "\n",
      "Topic: 12 \n",
      "Words: 0.172*\"pilot\" + 0.071*\"fuel\" + 0.070*\"save\" + 0.061*\"tower\" + 0.036*\"correct\" + 0.032*\"offens\" + 0.028*\"beat\" + 0.025*\"mode\" + 0.024*\"anchor\" + 0.018*\"ultim\"\n",
      "\n",
      "Topic: 13 \n",
      "Words: 0.111*\"choos\" + 0.074*\"fail\" + 0.045*\"concept\" + 0.039*\"stanc\" + 0.026*\"express\" + 0.025*\"readi\" + 0.025*\"flaw\" + 0.025*\"scale\" + 0.025*\"judg\" + 0.023*\"smart\"\n",
      "\n",
      "Topic: 14 \n",
      "Words: 0.118*\"highsec\" + 0.108*\"nullsec\" + 0.082*\"lowsec\" + 0.075*\"reward\" + 0.056*\"risk\" + 0.049*\"incurs\" + 0.027*\"player\" + 0.016*\"incom\" + 0.016*\"qualiti\" + 0.012*\"live\"\n",
      "\n",
      "Topic: 15 \n",
      "Words: 0.043*\"space\" + 0.043*\"system\" + 0.029*\"region\" + 0.024*\"structur\" + 0.022*\"pose\" + 0.019*\"station\" + 0.018*\"area\" + 0.017*\"control\" + 0.014*\"outpost\" + 0.014*\"need\"\n",
      "\n",
      "Topic: 16 \n",
      "Words: 0.898*\"support\" + 0.020*\"vocal\" + 0.019*\"violat\" + 0.005*\"need\" + 0.005*\"reason\" + 0.005*\"solari\" + 0.004*\"total\" + 0.004*\"drill\" + 0.002*\"edit\" + 0.002*\"nice\"\n",
      "\n",
      "Topic: 17 \n",
      "Words: 0.294*\"mission\" + 0.054*\"agent\" + 0.043*\"runner\" + 0.035*\"run\" + 0.034*\"level\" + 0.023*\"stand\" + 0.019*\"farm\" + 0.017*\"remind\" + 0.013*\"high\" + 0.012*\"peopl\"\n",
      "\n",
      "Topic: 18 \n",
      "Words: 0.104*\"second\" + 0.086*\"scan\" + 0.065*\"revamp\" + 0.054*\"probe\" + 0.050*\"drop\" + 0.044*\"anti\" + 0.039*\"delay\" + 0.033*\"automat\" + 0.032*\"clarifi\" + 0.030*\"cyno\"\n",
      "\n",
      "Topic: 19 \n",
      "Words: 0.135*\"mittani\" + 0.066*\"time\" + 0.064*\"choic\" + 0.046*\"ask\" + 0.045*\"wast\" + 0.036*\"liter\" + 0.030*\"whine\" + 0.024*\"dumb\" + 0.022*\"silli\" + 0.021*\"useless\"\n",
      "\n",
      "Topic: 20 \n",
      "Words: 0.272*\"propos\" + 0.057*\"annoy\" + 0.052*\"bpos\" + 0.044*\"pubbi\" + 0.039*\"notic\" + 0.033*\"blue\" + 0.029*\"freedom\" + 0.026*\"ripard\" + 0.025*\"aw\" + 0.025*\"origin\"\n",
      "\n",
      "Topic: 21 \n",
      "Words: 0.080*\"target\" + 0.043*\"ship\" + 0.028*\"size\" + 0.028*\"gun\" + 0.019*\"hit\" + 0.018*\"turret\" + 0.016*\"effect\" + 0.012*\"like\" + 0.012*\"damag\" + 0.012*\"calcul\"\n",
      "\n",
      "Topic: 22 \n",
      "Words: 0.069*\"rat\" + 0.037*\"special\" + 0.037*\"spawn\" + 0.025*\"averag\" + 0.023*\"npcs\" + 0.023*\"govern\" + 0.020*\"anomali\" + 0.020*\"excit\" + 0.018*\"gooni\" + 0.018*\"nation\"\n",
      "\n",
      "Topic: 23 \n",
      "Words: 0.100*\"gank\" + 0.063*\"ship\" + 0.055*\"risk\" + 0.051*\"lose\" + 0.037*\"suicid\" + 0.028*\"ganker\" + 0.025*\"insur\" + 0.024*\"loss\" + 0.014*\"blow\" + 0.012*\"kill\"\n",
      "\n",
      "Topic: 24 \n",
      "Words: 0.126*\"onlin\" + 0.056*\"thousand\" + 0.054*\"riverini\" + 0.050*\"input\" + 0.040*\"brain\" + 0.031*\"intern\" + 0.030*\"eye\" + 0.030*\"media\" + 0.028*\"confid\" + 0.025*\"utter\"\n",
      "\n",
      "Topic: 25 \n",
      "Words: 0.243*\"idea\" + 0.191*\"fffd\" + 0.151*\"good\" + 0.101*\"agre\" + 0.063*\"like\" + 0.051*\"think\" + 0.018*\"awesom\" + 0.012*\"thing\" + 0.009*\"look\" + 0.009*\"interest\"\n",
      "\n",
      "Topic: 26 \n",
      "Words: 0.115*\"modul\" + 0.066*\"tech\" + 0.054*\"missil\" + 0.025*\"ship\" + 0.025*\"bonus\" + 0.024*\"launcher\" + 0.023*\"heavi\" + 0.022*\"signatur\" + 0.019*\"reserv\" + 0.018*\"energi\"\n",
      "\n",
      "Topic: 27 \n",
      "Words: 0.042*\"group\" + 0.032*\"attack\" + 0.028*\"defend\" + 0.018*\"small\" + 0.017*\"larg\" + 0.015*\"forc\" + 0.015*\"blob\" + 0.013*\"smaller\" + 0.012*\"mechan\" + 0.012*\"larger\"\n",
      "\n",
      "Topic: 28 \n",
      "Words: 0.219*\"skill\" + 0.109*\"train\" + 0.047*\"level\" + 0.036*\"time\" + 0.026*\"player\" + 0.023*\"noob\" + 0.023*\"toon\" + 0.018*\"teach\" + 0.018*\"book\" + 0.017*\"point\"\n",
      "\n",
      "Topic: 29 \n",
      "Words: 0.193*\"nerf\" + 0.067*\"pick\" + 0.065*\"half\" + 0.061*\"explor\" + 0.052*\"imagin\" + 0.041*\"drink\" + 0.039*\"random\" + 0.038*\"today\" + 0.030*\"spot\" + 0.027*\"tabl\"\n",
      "\n",
      "Topic: 30 \n",
      "Words: 0.101*\"topic\" + 0.078*\"discuss\" + 0.060*\"mail\" + 0.058*\"check\" + 0.054*\"send\" + 0.051*\"forget\" + 0.050*\"mouth\" + 0.048*\"word\" + 0.039*\"messag\" + 0.037*\"fulli\"\n",
      "\n",
      "Topic: 31 \n",
      "Words: 0.091*\"fit\" + 0.057*\"jade\" + 0.048*\"mod\" + 0.044*\"ship\" + 0.044*\"drake\" + 0.041*\"crazi\" + 0.038*\"nano\" + 0.038*\"caldari\" + 0.034*\"speed\" + 0.032*\"gallent\"\n",
      "\n",
      "Topic: 32 \n",
      "Words: 0.209*\"space\" + 0.146*\"wormhol\" + 0.068*\"luck\" + 0.044*\"happi\" + 0.027*\"live\" + 0.027*\"hole\" + 0.026*\"star\" + 0.024*\"charg\" + 0.023*\"wall\" + 0.022*\"polici\"\n",
      "\n",
      "Topic: 33 \n",
      "Words: 0.084*\"organ\" + 0.073*\"request\" + 0.063*\"later\" + 0.044*\"reform\" + 0.042*\"applic\" + 0.035*\"bookmark\" + 0.032*\"largest\" + 0.025*\"refus\" + 0.025*\"stock\" + 0.020*\"struggl\"\n",
      "\n",
      "Topic: 34 \n",
      "Words: 0.061*\"uniqu\" + 0.051*\"ballot\" + 0.040*\"restrict\" + 0.033*\"credit\" + 0.031*\"wiki\" + 0.026*\"entri\" + 0.023*\"devil\" + 0.022*\"invest\" + 0.021*\"ensur\" + 0.021*\"barrier\"\n",
      "\n",
      "Topic: 35 \n",
      "Words: 0.131*\"campaign\" + 0.071*\"timer\" + 0.063*\"exploit\" + 0.040*\"mechan\" + 0.038*\"board\" + 0.035*\"offici\" + 0.033*\"oppos\" + 0.027*\"encourag\" + 0.018*\"leak\" + 0.017*\"pvper\"\n",
      "\n",
      "Topic: 36 \n",
      "Words: 0.392*\"chang\" + 0.024*\"return\" + 0.023*\"mechan\" + 0.021*\"recruit\" + 0.019*\"affect\" + 0.015*\"hop\" + 0.015*\"like\" + 0.014*\"hide\" + 0.013*\"gameplay\" + 0.013*\"think\"\n",
      "\n",
      "Topic: 37 \n",
      "Words: 0.166*\"friend\" + 0.108*\"wish\" + 0.063*\"crap\" + 0.061*\"salvag\" + 0.037*\"difficult\" + 0.027*\"felt\" + 0.027*\"sad\" + 0.025*\"prepar\" + 0.024*\"fool\" + 0.019*\"ninja\"\n",
      "\n",
      "Topic: 38 \n",
      "Words: 0.204*\"love\" + 0.075*\"hate\" + 0.044*\"csms\" + 0.039*\"impress\" + 0.036*\"motiv\" + 0.025*\"disappoint\" + 0.024*\"drama\" + 0.023*\"hack\" + 0.021*\"measur\" + 0.019*\"fundament\"\n",
      "\n",
      "Topic: 39 \n",
      "Words: 0.111*\"enjoy\" + 0.049*\"client\" + 0.049*\"news\" + 0.045*\"refer\" + 0.026*\"behavior\" + 0.025*\"hire\" + 0.024*\"respond\" + 0.022*\"style\" + 0.021*\"clean\" + 0.020*\"unabl\"\n",
      "\n",
      "Topic: 40 \n",
      "Words: 0.119*\"small\" + 0.081*\"argument\" + 0.067*\"solo\" + 0.058*\"gang\" + 0.039*\"argu\" + 0.038*\"advoc\" + 0.030*\"horribl\" + 0.026*\"ignor\" + 0.022*\"assum\" + 0.015*\"say\"\n",
      "\n",
      "Topic: 41 \n",
      "Words: 0.108*\"manag\" + 0.097*\"definit\" + 0.088*\"event\" + 0.060*\"histori\" + 0.036*\"integr\" + 0.035*\"search\" + 0.033*\"advanc\" + 0.033*\"transfer\" + 0.029*\"theme\" + 0.028*\"delet\"\n",
      "\n",
      "Topic: 42 \n",
      "Words: 0.291*\"vote\" + 0.125*\"candid\" + 0.089*\"elect\" + 0.054*\"repres\" + 0.023*\"voter\" + 0.021*\"peopl\" + 0.019*\"platform\" + 0.014*\"like\" + 0.012*\"best\" + 0.011*\"get\"\n",
      "\n",
      "Topic: 43 \n",
      "Words: 0.039*\"improv\" + 0.018*\"research\" + 0.017*\"current\" + 0.014*\"requir\" + 0.014*\"work\" + 0.013*\"thing\" + 0.012*\"specif\" + 0.011*\"method\" + 0.011*\"meta\" + 0.011*\"industrialist\"\n",
      "\n",
      "Topic: 44 \n",
      "Words: 0.183*\"drone\" + 0.111*\"damag\" + 0.075*\"bonus\" + 0.043*\"weapon\" + 0.042*\"type\" + 0.037*\"ammo\" + 0.021*\"ship\" + 0.017*\"hybrid\" + 0.010*\"urg\" + 0.009*\"need\"\n",
      "\n",
      "Topic: 45 \n",
      "Words: 0.193*\"game\" + 0.078*\"player\" + 0.067*\"play\" + 0.033*\"want\" + 0.033*\"peopl\" + 0.026*\"like\" + 0.019*\"thing\" + 0.018*\"think\" + 0.013*\"know\" + 0.013*\"mechan\"\n",
      "\n",
      "Topic: 46 \n",
      "Words: 0.119*\"speak\" + 0.114*\"object\" + 0.074*\"term\" + 0.059*\"deleg\" + 0.056*\"stori\" + 0.050*\"cover\" + 0.046*\"planet\" + 0.035*\"success\" + 0.029*\"amaz\" + 0.021*\"sink\"\n",
      "\n",
      "Topic: 47 \n",
      "Words: 0.083*\"jam\" + 0.076*\"slot\" + 0.053*\"chanc\" + 0.033*\"strength\" + 0.032*\"touch\" + 0.025*\"falcon\" + 0.025*\"stack\" + 0.022*\"bare\" + 0.022*\"ewar\" + 0.021*\"sensor\"\n",
      "\n",
      "Topic: 48 \n",
      "Words: 0.124*\"year\" + 0.063*\"month\" + 0.058*\"time\" + 0.049*\"plex\" + 0.040*\"day\" + 0.035*\"content\" + 0.026*\"spend\" + 0.019*\"go\" + 0.019*\"promis\" + 0.015*\"coupl\"\n",
      "\n",
      "Topic: 49 \n",
      "Words: 0.149*\"account\" + 0.085*\"dust\" + 0.066*\"bot\" + 0.050*\"leader\" + 0.047*\"damn\" + 0.044*\"report\" + 0.042*\"joke\" + 0.039*\"multipl\" + 0.025*\"doubt\" + 0.023*\"scream\"\n",
      "\n",
      "Topic: 50 \n",
      "Words: 0.283*\"activ\" + 0.127*\"hisec\" + 0.080*\"center\" + 0.034*\"generat\" + 0.033*\"command\" + 0.026*\"democrat\" + 0.024*\"dec\" + 0.023*\"portion\" + 0.020*\"organis\" + 0.018*\"passiv\"\n",
      "\n",
      "Topic: 51 \n",
      "Words: 0.135*\"scam\" + 0.071*\"fall\" + 0.037*\"legal\" + 0.034*\"lazi\" + 0.033*\"solid\" + 0.029*\"percentag\" + 0.028*\"children\" + 0.026*\"dismiss\" + 0.026*\"adapt\" + 0.023*\"girl\"\n",
      "\n",
      "Topic: 52 \n",
      "Words: 0.125*\"see\" + 0.119*\"major\" + 0.095*\"hard\" + 0.071*\"absolut\" + 0.043*\"logic\" + 0.041*\"realiz\" + 0.036*\"funni\" + 0.027*\"vast\" + 0.026*\"death\" + 0.026*\"resid\"\n",
      "\n",
      "Topic: 53 \n",
      "Words: 0.069*\"program\" + 0.060*\"anymor\" + 0.047*\"bomb\" + 0.045*\"vision\" + 0.042*\"evil\" + 0.040*\"ball\" + 0.033*\"enforc\" + 0.033*\"epic\" + 0.033*\"deep\" + 0.030*\"recal\"\n",
      "\n",
      "Topic: 54 \n",
      "Words: 0.086*\"player\" + 0.018*\"base\" + 0.015*\"member\" + 0.014*\"interest\" + 0.013*\"posit\" + 0.011*\"point\" + 0.011*\"think\" + 0.010*\"represent\" + 0.009*\"present\" + 0.009*\"goal\"\n",
      "\n",
      "Topic: 55 \n",
      "Words: 0.090*\"learn\" + 0.078*\"plan\" + 0.077*\"stupid\" + 0.052*\"newbi\" + 0.033*\"mistak\" + 0.030*\"burn\" + 0.027*\"decis\" + 0.026*\"declar\" + 0.025*\"experi\" + 0.020*\"intent\"\n",
      "\n",
      "Topic: 56 \n",
      "Words: 0.099*\"rule\" + 0.047*\"futur\" + 0.047*\"bother\" + 0.039*\"tear\" + 0.032*\"note\" + 0.031*\"idiot\" + 0.027*\"late\" + 0.025*\"apolog\" + 0.024*\"patch\" + 0.022*\"screw\"\n",
      "\n",
      "Topic: 57 \n",
      "Words: 0.122*\"fight\" + 0.117*\"fleet\" + 0.052*\"wardec\" + 0.035*\"join\" + 0.035*\"peopl\" + 0.029*\"engag\" + 0.022*\"enemi\" + 0.014*\"war\" + 0.014*\"roam\" + 0.013*\"kill\"\n",
      "\n",
      "Topic: 58 \n",
      "Words: 0.187*\"member\" + 0.114*\"corpor\" + 0.061*\"quot\" + 0.061*\"fix\" + 0.060*\"share\" + 0.045*\"team\" + 0.034*\"offic\" + 0.030*\"polit\" + 0.028*\"serv\" + 0.019*\"constitu\"\n",
      "\n",
      "Topic: 59 \n",
      "Words: 0.132*\"hand\" + 0.080*\"set\" + 0.077*\"user\" + 0.048*\"enabl\" + 0.048*\"handl\" + 0.047*\"kinda\" + 0.040*\"figur\" + 0.039*\"visit\" + 0.035*\"whilst\" + 0.031*\"imag\"\n",
      "\n",
      "Topic: 60 \n",
      "Words: 0.188*\"mine\" + 0.106*\"miner\" + 0.059*\"moon\" + 0.039*\"sit\" + 0.035*\"belt\" + 0.021*\"site\" + 0.017*\"asteroid\" + 0.016*\"hulk\" + 0.014*\"need\" + 0.014*\"barg\"\n",
      "\n",
      "Topic: 61 \n",
      "Words: 0.110*\"high\" + 0.039*\"secur\" + 0.038*\"pirat\" + 0.033*\"concord\" + 0.033*\"carebear\" + 0.031*\"status\" + 0.022*\"player\" + 0.021*\"safe\" + 0.020*\"protect\" + 0.017*\"space\"\n",
      "\n",
      "Topic: 62 \n",
      "Words: 0.339*\"corp\" + 0.040*\"interview\" + 0.036*\"access\" + 0.033*\"name\" + 0.024*\"member\" + 0.021*\"player\" + 0.020*\"log\" + 0.019*\"allow\" + 0.019*\"person\" + 0.015*\"steal\"\n",
      "\n",
      "Topic: 63 \n",
      "Words: 0.080*\"industri\" + 0.048*\"build\" + 0.035*\"cost\" + 0.035*\"miner\" + 0.027*\"product\" + 0.025*\"manufactur\" + 0.025*\"invent\" + 0.024*\"produc\" + 0.024*\"materi\" + 0.022*\"demand\"\n",
      "\n",
      "Topic: 64 \n",
      "Words: 0.152*\"hell\" + 0.134*\"self\" + 0.082*\"human\" + 0.068*\"destruct\" + 0.043*\"demonstr\" + 0.038*\"favour\" + 0.037*\"fill\" + 0.037*\"failur\" + 0.034*\"weak\" + 0.025*\"vice\"\n",
      "\n",
      "Topic: 65 \n",
      "Words: 0.047*\"peopl\" + 0.026*\"expect\" + 0.024*\"say\" + 0.023*\"claim\" + 0.021*\"think\" + 0.021*\"cours\" + 0.020*\"action\" + 0.019*\"thing\" + 0.019*\"fact\" + 0.018*\"know\"\n",
      "\n",
      "Topic: 66 \n",
      "Words: 0.131*\"hear\" + 0.079*\"yeah\" + 0.042*\"say\" + 0.040*\"chair\" + 0.029*\"eden\" + 0.028*\"like\" + 0.028*\"gonna\" + 0.028*\"focus\" + 0.025*\"past\" + 0.023*\"copi\"\n",
      "\n",
      "Topic: 67 \n",
      "Words: 0.108*\"servic\" + 0.107*\"product\" + 0.104*\"core\" + 0.065*\"glad\" + 0.047*\"accord\" + 0.041*\"consist\" + 0.040*\"repair\" + 0.034*\"part\" + 0.032*\"host\" + 0.023*\"magic\"\n",
      "\n",
      "Topic: 68 \n",
      "Words: 0.155*\"problem\" + 0.059*\"like\" + 0.057*\"sound\" + 0.056*\"sens\" + 0.050*\"make\" + 0.043*\"solut\" + 0.035*\"explain\" + 0.025*\"solv\" + 0.024*\"good\" + 0.017*\"think\"\n",
      "\n",
      "Topic: 69 \n",
      "Words: 0.167*\"ship\" + 0.045*\"balanc\" + 0.041*\"role\" + 0.026*\"class\" + 0.019*\"counter\" + 0.013*\"need\" + 0.013*\"combat\" + 0.010*\"think\" + 0.009*\"stat\" + 0.009*\"better\"\n",
      "\n",
      "Topic: 70 \n",
      "Words: 0.126*\"bloc\" + 0.064*\"fanfest\" + 0.054*\"hello\" + 0.050*\"round\" + 0.036*\"mike\" + 0.030*\"occur\" + 0.029*\"skin\" + 0.028*\"shift\" + 0.026*\"merit\" + 0.021*\"sansha\"\n",
      "\n",
      "Topic: 71 \n",
      "Words: 0.181*\"troll\" + 0.129*\"jump\" + 0.129*\"gate\" + 0.061*\"lock\" + 0.046*\"camp\" + 0.037*\"bubbl\" + 0.028*\"travel\" + 0.019*\"drive\" + 0.012*\"like\" + 0.011*\"gun\"\n",
      "\n",
      "Topic: 72 \n",
      "Words: 0.065*\"consequ\" + 0.056*\"neutral\" + 0.054*\"massiv\" + 0.047*\"promot\" + 0.047*\"logi\" + 0.042*\"flag\" + 0.035*\"aggress\" + 0.033*\"extend\" + 0.032*\"navi\" + 0.030*\"speech\"\n",
      "\n",
      "Topic: 73 \n",
      "Words: 0.253*\"order\" + 0.131*\"updat\" + 0.060*\"loot\" + 0.056*\"wreck\" + 0.032*\"kid\" + 0.026*\"buy\" + 0.022*\"satisfi\" + 0.017*\"scrap\" + 0.017*\"cut\" + 0.016*\"driver\"\n",
      "\n",
      "Topic: 74 \n",
      "Words: 0.175*\"station\" + 0.138*\"option\" + 0.065*\"video\" + 0.059*\"incarna\" + 0.045*\"warn\" + 0.037*\"undock\" + 0.037*\"walk\" + 0.035*\"load\" + 0.027*\"ship\" + 0.024*\"eula\"\n",
      "\n",
      "Topic: 75 \n",
      "Words: 0.117*\"link\" + 0.047*\"intend\" + 0.042*\"appreci\" + 0.039*\"subject\" + 0.036*\"knowledg\" + 0.030*\"work\" + 0.030*\"educ\" + 0.027*\"playerbas\" + 0.026*\"section\" + 0.026*\"cross\"\n",
      "\n",
      "Topic: 76 \n",
      "Words: 0.151*\"real\" + 0.096*\"life\" + 0.065*\"world\" + 0.037*\"univers\" + 0.027*\"game\" + 0.021*\"compani\" + 0.017*\"intellig\" + 0.016*\"dark\" + 0.014*\"dear\" + 0.013*\"countri\"\n",
      "\n",
      "Topic: 77 \n",
      "Words: 0.083*\"communiti\" + 0.038*\"meet\" + 0.034*\"communic\" + 0.033*\"blog\" + 0.022*\"iceland\" + 0.019*\"busi\" + 0.019*\"discuss\" + 0.014*\"strong\" + 0.014*\"forum\" + 0.013*\"player\"\n",
      "\n",
      "Topic: 78 \n",
      "Words: 0.050*\"market\" + 0.041*\"money\" + 0.033*\"price\" + 0.032*\"sell\" + 0.025*\"goonswarm\" + 0.023*\"trade\" + 0.023*\"peopl\" + 0.018*\"player\" + 0.017*\"profit\" + 0.012*\"cost\"\n",
      "\n",
      "Topic: 79 \n",
      "Words: 0.160*\"dont\" + 0.081*\"sign\" + 0.072*\"miss\" + 0.071*\"care\" + 0.064*\"endors\" + 0.046*\"listen\" + 0.042*\"malcani\" + 0.041*\"write\" + 0.039*\"mess\" + 0.032*\"wont\"\n",
      "\n",
      "Topic: 80 \n",
      "Words: 0.160*\"break\" + 0.132*\"featur\" + 0.102*\"twitter\" + 0.075*\"dock\" + 0.069*\"tool\" + 0.048*\"data\" + 0.024*\"mark\" + 0.024*\"advertis\" + 0.020*\"lore\" + 0.017*\"daili\"\n",
      "\n",
      "Topic: 81 \n",
      "Words: 0.231*\"stuff\" + 0.104*\"power\" + 0.085*\"han\" + 0.045*\"grid\" + 0.034*\"field\" + 0.028*\"night\" + 0.028*\"angri\" + 0.016*\"control\" + 0.016*\"wide\" + 0.014*\"metagam\"\n",
      "\n",
      "Topic: 82 \n",
      "Words: 0.166*\"shoot\" + 0.134*\"butt\" + 0.077*\"benefit\" + 0.067*\"prove\" + 0.038*\"leadership\" + 0.026*\"justifi\" + 0.022*\"anonym\" + 0.020*\"attribut\" + 0.020*\"equal\" + 0.019*\"chain\"\n",
      "\n",
      "Topic: 83 \n",
      "Words: 0.120*\"cloak\" + 0.052*\"ship\" + 0.042*\"black\" + 0.023*\"cloaki\" + 0.023*\"cloaker\" + 0.020*\"bomber\" + 0.018*\"covert\" + 0.014*\"immun\" + 0.013*\"need\" + 0.012*\"stealth\"\n",
      "\n",
      "Topic: 84 \n",
      "Words: 0.197*\"allianc\" + 0.120*\"null\" + 0.023*\"darius\" + 0.022*\"space\" + 0.019*\"want\" + 0.015*\"live\" + 0.015*\"empir\" + 0.014*\"confirm\" + 0.014*\"hold\" + 0.013*\"larg\"\n",
      "\n",
      "Topic: 85 \n",
      "Words: 0.092*\"titan\" + 0.077*\"capit\" + 0.065*\"super\" + 0.040*\"carrier\" + 0.037*\"jump\" + 0.034*\"ship\" + 0.034*\"bridg\" + 0.032*\"supercap\" + 0.022*\"fleet\" + 0.021*\"cyno\"\n",
      "\n",
      "Topic: 86 \n",
      "Words: 0.194*\"wait\" + 0.133*\"expans\" + 0.108*\"bump\" + 0.076*\"spaceship\" + 0.060*\"assembl\" + 0.058*\"hall\" + 0.053*\"surpris\" + 0.051*\"welcom\" + 0.037*\"cap\" + 0.034*\"begin\"\n",
      "\n",
      "Topic: 87 \n",
      "Words: 0.125*\"post\" + 0.087*\"thread\" + 0.053*\"forum\" + 0.048*\"question\" + 0.046*\"read\" + 0.036*\"answer\" + 0.017*\"know\" + 0.017*\"think\" + 0.017*\"like\" + 0.016*\"peopl\"\n",
      "\n",
      "Topic: 88 \n",
      "Words: 0.131*\"local\" + 0.089*\"remov\" + 0.082*\"stand\" + 0.058*\"chat\" + 0.056*\"channel\" + 0.030*\"couldn\" + 0.030*\"intel\" + 0.018*\"shall\" + 0.017*\"roleplay\" + 0.017*\"peopl\"\n",
      "\n",
      "Topic: 89 \n",
      "Words: 0.060*\"public\" + 0.053*\"peopl\" + 0.045*\"disagre\" + 0.041*\"contract\" + 0.040*\"look\" + 0.040*\"want\" + 0.038*\"forward\" + 0.037*\"internet\" + 0.036*\"complain\" + 0.031*\"quit\"\n",
      "\n",
      "Topic: 90 \n",
      "Words: 0.113*\"page\" + 0.099*\"council\" + 0.083*\"summit\" + 0.054*\"dedic\" + 0.043*\"doubl\" + 0.040*\"kelduum\" + 0.038*\"modular\" + 0.031*\"convers\" + 0.030*\"pointless\" + 0.027*\"manag\"\n",
      "\n",
      "Topic: 91 \n",
      "Words: 0.189*\"issu\" + 0.023*\"rais\" + 0.023*\"releas\" + 0.020*\"discuss\" + 0.018*\"think\" + 0.018*\"pass\" + 0.015*\"debat\" + 0.015*\"bring\" + 0.013*\"agenda\" + 0.012*\"work\"\n",
      "\n",
      "Topic: 92 \n",
      "Words: 0.294*\"charact\" + 0.060*\"screen\" + 0.048*\"account\" + 0.048*\"avatar\" + 0.047*\"confus\" + 0.047*\"pictur\" + 0.042*\"text\" + 0.021*\"elig\" + 0.015*\"creation\" + 0.015*\"creat\"\n",
      "\n",
      "Topic: 93 \n",
      "Words: 0.080*\"rang\" + 0.066*\"track\" + 0.045*\"amarr\" + 0.034*\"race\" + 0.034*\"minmatar\" + 0.024*\"close\" + 0.022*\"optim\" + 0.017*\"caldari\" + 0.017*\"gallent\" + 0.016*\"blaster\"\n",
      "\n",
      "Topic: 94 \n",
      "Words: 0.110*\"talk\" + 0.071*\"know\" + 0.060*\"test\" + 0.044*\"stay\" + 0.043*\"server\" + 0.041*\"deserv\" + 0.033*\"attent\" + 0.032*\"tell\" + 0.031*\"kick\" + 0.028*\"call\"\n",
      "\n",
      "Topic: 95 \n",
      "Words: 0.108*\"hour\" + 0.098*\"minut\" + 0.072*\"run\" + 0.068*\"time\" + 0.057*\"altern\" + 0.049*\"watch\" + 0.043*\"week\" + 0.041*\"statement\" + 0.029*\"excel\" + 0.028*\"defin\"\n",
      "\n",
      "Topic: 96 \n",
      "Words: 0.085*\"frigat\" + 0.077*\"wonder\" + 0.067*\"cruiser\" + 0.041*\"battleship\" + 0.038*\"ship\" + 0.038*\"fli\" + 0.035*\"plus\" + 0.032*\"elis\" + 0.031*\"frig\" + 0.023*\"rifter\"\n",
      "\n",
      "Topic: 97 \n",
      "Words: 0.252*\"great\" + 0.074*\"terribl\" + 0.039*\"cycl\" + 0.039*\"pure\" + 0.038*\"poster\" + 0.034*\"connect\" + 0.028*\"contribut\" + 0.025*\"unfortun\" + 0.022*\"die\" + 0.022*\"indic\"\n",
      "\n",
      "Topic: 98 \n",
      "Words: 0.115*\"list\" + 0.075*\"view\" + 0.052*\"click\" + 0.048*\"button\" + 0.040*\"select\" + 0.039*\"window\" + 0.026*\"contact\" + 0.026*\"press\" + 0.024*\"right\" + 0.018*\"like\"\n",
      "\n",
      "Topic: 99 \n",
      "Words: 0.101*\"shouldn\" + 0.093*\"clone\" + 0.070*\"implant\" + 0.065*\"seleen\" + 0.057*\"regard\" + 0.027*\"pod\" + 0.025*\"expens\" + 0.024*\"split\" + 0.024*\"afford\" + 0.021*\"cost\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running LDA using TF-IDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.023*\"steal\" + 0.021*\"transpar\" + 0.018*\"land\" + 0.017*\"suspect\" + 0.017*\"amber\" + 0.015*\"dismiss\" + 0.014*\"loot\" + 0.014*\"tell\" + 0.013*\"mutual\" + 0.013*\"playerbas\"\n",
      "Topic: 1 Word: 0.012*\"surpris\" + 0.009*\"aggro\" + 0.009*\"game\" + 0.008*\"player\" + 0.008*\"protest\" + 0.008*\"guid\" + 0.008*\"chicken\" + 0.007*\"adapt\" + 0.006*\"peopl\" + 0.006*\"morn\"\n",
      "Topic: 2 Word: 0.028*\"request\" + 0.025*\"congratul\" + 0.022*\"amaz\" + 0.016*\"moron\" + 0.016*\"squad\" + 0.014*\"monocl\" + 0.012*\"integr\" + 0.011*\"bold\" + 0.010*\"connect\" + 0.010*\"colour\"\n",
      "Topic: 3 Word: 0.034*\"want\" + 0.034*\"killmail\" + 0.022*\"skin\" + 0.019*\"iron\" + 0.018*\"tutori\" + 0.016*\"weird\" + 0.015*\"sub\" + 0.015*\"finger\" + 0.015*\"middl\" + 0.014*\"kitti\"\n",
      "Topic: 4 Word: 0.020*\"ship\" + 0.020*\"cyno\" + 0.017*\"frigat\" + 0.016*\"cruiser\" + 0.014*\"drink\" + 0.011*\"battleship\" + 0.011*\"covert\" + 0.010*\"disappoint\" + 0.010*\"revamp\" + 0.010*\"round\"\n",
      "Topic: 5 Word: 0.063*\"skill\" + 0.052*\"train\" + 0.033*\"learn\" + 0.017*\"charact\" + 0.017*\"time\" + 0.016*\"agenda\" + 0.014*\"attribut\" + 0.014*\"game\" + 0.013*\"play\" + 0.011*\"month\"\n",
      "Topic: 6 Word: 0.048*\"sorri\" + 0.046*\"malcani\" + 0.040*\"summit\" + 0.030*\"suck\" + 0.026*\"congrat\" + 0.022*\"bridg\" + 0.022*\"search\" + 0.020*\"region\" + 0.020*\"cross\" + 0.019*\"hasn\"\n",
      "Topic: 7 Word: 0.230*\"agre\" + 0.104*\"spam\" + 0.036*\"total\" + 0.033*\"bookmark\" + 0.027*\"freedom\" + 0.026*\"upcom\" + 0.025*\"viewpoint\" + 0.021*\"file\" + 0.019*\"accur\" + 0.016*\"trash\"\n",
      "Topic: 8 Word: 0.081*\"yeah\" + 0.060*\"voter\" + 0.044*\"rare\" + 0.030*\"updat\" + 0.028*\"thought\" + 0.026*\"wonder\" + 0.025*\"write\" + 0.024*\"ripard\" + 0.019*\"bear\" + 0.019*\"jita\"\n",
      "Topic: 9 Word: 0.728*\"sign\" + 0.036*\"fri\" + 0.029*\"turnout\" + 0.026*\"hat\" + 0.020*\"donat\" + 0.018*\"dare\" + 0.015*\"happi\" + 0.015*\"campaign\" + 0.013*\"concur\" + 0.011*\"irrit\"\n",
      "Topic: 10 Word: 0.019*\"represent\" + 0.013*\"log\" + 0.012*\"vote\" + 0.012*\"late\" + 0.010*\"player\" + 0.010*\"repres\" + 0.009*\"author\" + 0.009*\"game\" + 0.008*\"school\" + 0.008*\"contest\"\n",
      "Topic: 11 Word: 0.685*\"support\" + 0.050*\"nice\" + 0.026*\"contribut\" + 0.026*\"idea\" + 0.016*\"propos\" + 0.015*\"winter\" + 0.014*\"piti\" + 0.010*\"thread\" + 0.009*\"necro\" + 0.007*\"plane\"\n",
      "Topic: 12 Word: 0.049*\"contract\" + 0.021*\"account\" + 0.020*\"corrupt\" + 0.020*\"manifesto\" + 0.019*\"popul\" + 0.019*\"pretend\" + 0.017*\"stock\" + 0.016*\"market\" + 0.016*\"clue\" + 0.015*\"excus\"\n",
      "Topic: 13 Word: 0.051*\"relev\" + 0.045*\"explain\" + 0.038*\"decis\" + 0.033*\"aw\" + 0.027*\"moar\" + 0.025*\"hous\" + 0.024*\"face\" + 0.024*\"poor\" + 0.021*\"conclus\" + 0.021*\"refus\"\n",
      "Topic: 14 Word: 0.050*\"deleg\" + 0.038*\"supercap\" + 0.033*\"ball\" + 0.032*\"grow\" + 0.029*\"download\" + 0.028*\"demonstr\" + 0.028*\"belief\" + 0.027*\"mean\" + 0.024*\"insan\" + 0.022*\"whiner\"\n",
      "Topic: 15 Word: 0.042*\"sound\" + 0.011*\"publish\" + 0.011*\"like\" + 0.010*\"good\" + 0.009*\"epic\" + 0.008*\"farmer\" + 0.008*\"silenc\" + 0.007*\"elig\" + 0.007*\"sansha\" + 0.006*\"rise\"\n",
      "Topic: 16 Word: 0.034*\"dust\" + 0.029*\"fix\" + 0.024*\"communic\" + 0.019*\"moder\" + 0.017*\"need\" + 0.014*\"email\" + 0.012*\"dislik\" + 0.009*\"anonym\" + 0.009*\"game\" + 0.008*\"compress\"\n",
      "Topic: 17 Word: 0.049*\"night\" + 0.045*\"chribba\" + 0.044*\"privat\" + 0.043*\"reset\" + 0.042*\"translat\" + 0.034*\"cosmet\" + 0.034*\"faucet\" + 0.029*\"permiss\" + 0.028*\"harm\" + 0.026*\"renam\"\n",
      "Topic: 18 Word: 0.016*\"social\" + 0.014*\"sad\" + 0.014*\"hack\" + 0.012*\"haha\" + 0.010*\"town\" + 0.009*\"schedul\" + 0.009*\"sick\" + 0.009*\"behaviour\" + 0.008*\"mock\" + 0.007*\"game\"\n",
      "Topic: 19 Word: 0.030*\"tech\" + 0.029*\"moon\" + 0.024*\"dumb\" + 0.016*\"toon\" + 0.013*\"tinfoil\" + 0.012*\"convers\" + 0.011*\"mine\" + 0.008*\"crystal\" + 0.007*\"miner\" + 0.007*\"comm\"\n",
      "Topic: 20 Word: 0.070*\"endors\" + 0.040*\"warn\" + 0.039*\"glad\" + 0.035*\"parent\" + 0.032*\"idiot\" + 0.029*\"repres\" + 0.025*\"button\" + 0.021*\"obvious\" + 0.019*\"radio\" + 0.014*\"resid\"\n",
      "Topic: 21 Word: 0.038*\"kelduum\" + 0.033*\"gallent\" + 0.028*\"amarr\" + 0.028*\"caldari\" + 0.022*\"sink\" + 0.021*\"race\" + 0.021*\"boat\" + 0.021*\"violat\" + 0.019*\"doesnt\" + 0.019*\"close\"\n",
      "Topic: 22 Word: 0.018*\"titan\" + 0.015*\"ship\" + 0.012*\"carrier\" + 0.011*\"capit\" + 0.011*\"fleet\" + 0.010*\"super\" + 0.008*\"dread\" + 0.007*\"gang\" + 0.007*\"cap\" + 0.006*\"small\"\n",
      "Topic: 23 Word: 0.044*\"goonswarm\" + 0.039*\"chairman\" + 0.018*\"document\" + 0.018*\"eula\" + 0.014*\"clarifi\" + 0.013*\"wasn\" + 0.011*\"leak\" + 0.009*\"interact\" + 0.009*\"assumpt\" + 0.008*\"orbit\"\n",
      "Topic: 24 Word: 0.087*\"mittani\" + 0.014*\"disagre\" + 0.012*\"vote\" + 0.011*\"pain\" + 0.010*\"goon\" + 0.009*\"punish\" + 0.009*\"seek\" + 0.009*\"propaganda\" + 0.008*\"spirit\" + 0.007*\"life\"\n",
      "Topic: 25 Word: 0.026*\"outpost\" + 0.023*\"test\" + 0.015*\"theme\" + 0.014*\"recommend\" + 0.014*\"grief\" + 0.014*\"ring\" + 0.013*\"neutral\" + 0.012*\"disabl\" + 0.011*\"tire\" + 0.010*\"zone\"\n",
      "Topic: 26 Word: 0.040*\"click\" + 0.035*\"bpos\" + 0.026*\"program\" + 0.023*\"lazi\" + 0.020*\"press\" + 0.020*\"destin\" + 0.018*\"passion\" + 0.015*\"sleep\" + 0.014*\"complic\" + 0.013*\"script\"\n",
      "Topic: 27 Word: 0.039*\"doom\" + 0.037*\"code\" + 0.029*\"resourc\" + 0.028*\"jerk\" + 0.027*\"ultim\" + 0.026*\"remov\" + 0.026*\"intellig\" + 0.025*\"vice\" + 0.023*\"poke\" + 0.021*\"gateway\"\n",
      "Topic: 28 Word: 0.053*\"local\" + 0.032*\"dock\" + 0.027*\"station\" + 0.025*\"intel\" + 0.021*\"undock\" + 0.017*\"delay\" + 0.017*\"timer\" + 0.012*\"beam\" + 0.011*\"tool\" + 0.011*\"fool\"\n",
      "Topic: 29 Word: 0.074*\"post\" + 0.062*\"topic\" + 0.062*\"interview\" + 0.052*\"thread\" + 0.034*\"lock\" + 0.029*\"forum\" + 0.020*\"section\" + 0.019*\"doubl\" + 0.015*\"best\" + 0.011*\"thank\"\n",
      "Topic: 30 Word: 0.092*\"han\" + 0.040*\"visit\" + 0.038*\"imag\" + 0.034*\"user\" + 0.030*\"set\" + 0.028*\"enabl\" + 0.026*\"lie\" + 0.026*\"mike\" + 0.021*\"popular\" + 0.021*\"devblog\"\n",
      "Topic: 31 Word: 0.041*\"defin\" + 0.037*\"qualifi\" + 0.032*\"hero\" + 0.027*\"raven\" + 0.025*\"explicit\" + 0.025*\"ratter\" + 0.021*\"varieti\" + 0.021*\"cloud\" + 0.017*\"classic\" + 0.017*\"infinit\"\n",
      "Topic: 32 Word: 0.025*\"plex\" + 0.021*\"thumb\" + 0.018*\"riverini\" + 0.018*\"price\" + 0.016*\"democraci\" + 0.012*\"sell\" + 0.012*\"market\" + 0.011*\"money\" + 0.010*\"game\" + 0.010*\"buy\"\n",
      "Topic: 33 Word: 0.074*\"step\" + 0.046*\"truth\" + 0.042*\"babi\" + 0.037*\"appar\" + 0.029*\"occup\" + 0.028*\"format\" + 0.024*\"citizen\" + 0.019*\"depth\" + 0.019*\"analog\" + 0.018*\"deklein\"\n",
      "Topic: 34 Word: 0.018*\"accus\" + 0.017*\"curious\" + 0.014*\"player\" + 0.013*\"conspiraci\" + 0.012*\"million\" + 0.011*\"shock\" + 0.010*\"newer\" + 0.009*\"noob\" + 0.009*\"older\" + 0.009*\"game\"\n",
      "Topic: 35 Word: 0.043*\"meet\" + 0.032*\"jade\" + 0.027*\"iceland\" + 0.023*\"issu\" + 0.019*\"minut\" + 0.019*\"rais\" + 0.018*\"delet\" + 0.018*\"excel\" + 0.014*\"discuss\" + 0.013*\"vote\"\n",
      "Topic: 36 Word: 0.021*\"voic\" + 0.015*\"wall\" + 0.014*\"motiv\" + 0.011*\"music\" + 0.009*\"back\" + 0.009*\"breach\" + 0.008*\"soon\" + 0.008*\"legit\" + 0.008*\"vote\" + 0.008*\"guarante\"\n",
      "Topic: 37 Word: 0.043*\"gank\" + 0.028*\"insur\" + 0.028*\"suicid\" + 0.021*\"concord\" + 0.018*\"ganker\" + 0.017*\"newbi\" + 0.011*\"high\" + 0.011*\"hulk\" + 0.011*\"ship\" + 0.009*\"risk\"\n",
      "Topic: 38 Word: 0.132*\"mitten\" + 0.058*\"tear\" + 0.028*\"playstyl\" + 0.024*\"vision\" + 0.021*\"blind\" + 0.020*\"farm\" + 0.019*\"anim\" + 0.017*\"poast\" + 0.017*\"failur\" + 0.017*\"prohibit\"\n",
      "Topic: 39 Word: 0.048*\"mail\" + 0.027*\"dear\" + 0.027*\"send\" + 0.024*\"judg\" + 0.019*\"tabl\" + 0.018*\"happen\" + 0.018*\"afraid\" + 0.018*\"cheer\" + 0.017*\"impli\" + 0.014*\"beer\"\n",
      "Topic: 40 Word: 0.025*\"client\" + 0.024*\"server\" + 0.014*\"rule\" + 0.014*\"smart\" + 0.014*\"host\" + 0.014*\"ambul\" + 0.013*\"spawn\" + 0.013*\"survey\" + 0.012*\"favorit\" + 0.011*\"load\"\n",
      "Topic: 41 Word: 0.033*\"offici\" + 0.031*\"exploit\" + 0.029*\"laugh\" + 0.018*\"hurt\" + 0.017*\"serious\" + 0.017*\"bodi\" + 0.016*\"know\" + 0.015*\"butt\" + 0.015*\"pant\" + 0.014*\"kinda\"\n",
      "Topic: 42 Word: 0.014*\"casual\" + 0.013*\"book\" + 0.012*\"ninja\" + 0.011*\"okay\" + 0.011*\"iter\" + 0.010*\"player\" + 0.008*\"flip\" + 0.008*\"duti\" + 0.007*\"medal\" + 0.007*\"game\"\n",
      "Topic: 43 Word: 0.084*\"confirm\" + 0.064*\"hear\" + 0.063*\"miss\" + 0.047*\"sure\" + 0.036*\"haven\" + 0.033*\"white\" + 0.031*\"quit\" + 0.030*\"black\" + 0.030*\"rock\" + 0.026*\"date\"\n",
      "Topic: 44 Word: 0.082*\"luck\" + 0.050*\"hisec\" + 0.043*\"reserv\" + 0.037*\"jam\" + 0.034*\"secretari\" + 0.032*\"good\" + 0.028*\"altern\" + 0.026*\"vote\" + 0.025*\"proof\" + 0.024*\"deleg\"\n",
      "Topic: 45 Word: 0.035*\"terribl\" + 0.032*\"stanc\" + 0.017*\"blame\" + 0.016*\"applic\" + 0.016*\"silli\" + 0.015*\"proud\" + 0.013*\"interfac\" + 0.011*\"pathet\" + 0.011*\"static\" + 0.009*\"tough\"\n",
      "Topic: 46 Word: 0.040*\"mission\" + 0.028*\"faction\" + 0.025*\"stand\" + 0.023*\"agent\" + 0.021*\"reward\" + 0.013*\"risk\" + 0.011*\"high\" + 0.011*\"session\" + 0.010*\"warfar\" + 0.010*\"empir\"\n",
      "Topic: 47 Word: 0.073*\"bounti\" + 0.059*\"lowsec\" + 0.053*\"highsec\" + 0.041*\"awesom\" + 0.037*\"pirat\" + 0.026*\"hunt\" + 0.021*\"kill\" + 0.018*\"rat\" + 0.016*\"belt\" + 0.014*\"hunter\"\n",
      "Topic: 48 Word: 0.113*\"candid\" + 0.066*\"fffd\" + 0.025*\"campaign\" + 0.020*\"elect\" + 0.020*\"evemail\" + 0.013*\"leader\" + 0.006*\"know\" + 0.006*\"think\" + 0.006*\"confus\" + 0.006*\"stop\"\n",
      "Topic: 49 Word: 0.016*\"angri\" + 0.013*\"evid\" + 0.012*\"polici\" + 0.011*\"summari\" + 0.009*\"director\" + 0.009*\"rant\" + 0.009*\"peopl\" + 0.009*\"expans\" + 0.009*\"game\" + 0.009*\"univers\"\n",
      "Topic: 50 Word: 0.035*\"seleen\" + 0.028*\"joke\" + 0.021*\"window\" + 0.019*\"absolut\" + 0.016*\"abstain\" + 0.015*\"overview\" + 0.014*\"screen\" + 0.013*\"correct\" + 0.013*\"spell\" + 0.013*\"countri\"\n",
      "Topic: 51 Word: 0.104*\"darius\" + 0.091*\"great\" + 0.050*\"link\" + 0.036*\"bomb\" + 0.034*\"kick\" + 0.032*\"king\" + 0.022*\"idea\" + 0.022*\"booster\" + 0.019*\"irrelev\" + 0.018*\"anyway\"\n",
      "Topic: 52 Word: 0.031*\"poster\" + 0.030*\"mate\" + 0.025*\"attent\" + 0.025*\"titl\" + 0.022*\"democrat\" + 0.019*\"somebodi\" + 0.016*\"excit\" + 0.016*\"cheat\" + 0.015*\"quiet\" + 0.013*\"critic\"\n",
      "Topic: 53 Word: 0.057*\"liter\" + 0.029*\"sandbox\" + 0.026*\"legal\" + 0.020*\"readi\" + 0.019*\"recal\" + 0.017*\"databas\" + 0.016*\"matur\" + 0.016*\"trader\" + 0.015*\"imagin\" + 0.015*\"tier\"\n",
      "Topic: 54 Word: 0.045*\"cloak\" + 0.018*\"elis\" + 0.015*\"cloaker\" + 0.013*\"camp\" + 0.013*\"gate\" + 0.011*\"cloaki\" + 0.010*\"ship\" + 0.008*\"bias\" + 0.008*\"local\" + 0.007*\"peopl\"\n",
      "Topic: 55 Word: 0.117*\"good\" + 0.098*\"like\" + 0.091*\"idea\" + 0.055*\"forget\" + 0.047*\"mouth\" + 0.046*\"seat\" + 0.029*\"english\" + 0.027*\"thread\" + 0.023*\"look\" + 0.020*\"stuff\"\n",
      "Topic: 56 Word: 0.045*\"repli\" + 0.041*\"say\" + 0.027*\"hello\" + 0.026*\"gooni\" + 0.019*\"room\" + 0.017*\"public\" + 0.016*\"unsub\" + 0.016*\"victori\" + 0.014*\"victim\" + 0.013*\"dirti\"\n",
      "Topic: 57 Word: 0.027*\"bot\" + 0.027*\"pose\" + 0.023*\"tower\" + 0.019*\"fuel\" + 0.018*\"structur\" + 0.016*\"avatar\" + 0.014*\"anchor\" + 0.013*\"hangar\" + 0.013*\"blue\" + 0.013*\"couldn\"\n",
      "Topic: 58 Word: 0.089*\"core\" + 0.044*\"mine\" + 0.040*\"carebear\" + 0.028*\"laser\" + 0.025*\"version\" + 0.022*\"principl\" + 0.020*\"stabl\" + 0.020*\"earth\" + 0.019*\"threaten\" + 0.017*\"bare\"\n",
      "Topic: 59 Word: 0.044*\"petit\" + 0.036*\"ignor\" + 0.027*\"remind\" + 0.023*\"assum\" + 0.021*\"helpless\" + 0.021*\"submit\" + 0.020*\"patch\" + 0.020*\"repair\" + 0.018*\"blah\" + 0.017*\"option\"\n",
      "Topic: 60 Word: 0.049*\"drone\" + 0.043*\"missil\" + 0.027*\"damag\" + 0.024*\"rang\" + 0.018*\"weapon\" + 0.017*\"bonus\" + 0.017*\"gun\" + 0.017*\"turret\" + 0.017*\"ammo\" + 0.016*\"track\"\n",
      "Topic: 61 Word: 0.056*\"bloc\" + 0.041*\"report\" + 0.038*\"burn\" + 0.035*\"azariah\" + 0.019*\"navi\" + 0.017*\"arena\" + 0.017*\"pretti\" + 0.016*\"today\" + 0.016*\"vagu\" + 0.015*\"barg\"\n",
      "Topic: 62 Word: 0.044*\"question\" + 0.042*\"answer\" + 0.016*\"contact\" + 0.012*\"contain\" + 0.012*\"sentenc\" + 0.010*\"strang\" + 0.010*\"statist\" + 0.008*\"barbi\" + 0.007*\"worthless\" + 0.007*\"hub\"\n",
      "Topic: 63 Word: 0.034*\"corp\" + 0.031*\"wardec\" + 0.021*\"approv\" + 0.016*\"corpor\" + 0.012*\"player\" + 0.010*\"highsec\" + 0.008*\"join\" + 0.008*\"peopl\" + 0.007*\"asset\" + 0.007*\"pvper\"\n",
      "Topic: 64 Word: 0.070*\"trebor\" + 0.056*\"council\" + 0.029*\"deserv\" + 0.026*\"member\" + 0.024*\"vote\" + 0.023*\"exact\" + 0.018*\"context\" + 0.017*\"dweller\" + 0.014*\"adequ\" + 0.013*\"russian\"\n",
      "Topic: 65 Word: 0.050*\"damn\" + 0.042*\"whine\" + 0.042*\"prove\" + 0.029*\"record\" + 0.027*\"dream\" + 0.023*\"upset\" + 0.022*\"serv\" + 0.021*\"lawyer\" + 0.020*\"tomorrow\" + 0.019*\"justifi\"\n",
      "Topic: 66 Word: 0.085*\"twitter\" + 0.072*\"blog\" + 0.021*\"coalit\" + 0.018*\"filter\" + 0.017*\"background\" + 0.016*\"dark\" + 0.016*\"rig\" + 0.012*\"isnt\" + 0.012*\"censor\" + 0.011*\"iirc\"\n",
      "Topic: 67 Word: 0.069*\"salvag\" + 0.044*\"wreck\" + 0.038*\"guy\" + 0.034*\"announc\" + 0.025*\"didnt\" + 0.024*\"death\" + 0.023*\"manipul\" + 0.022*\"day\" + 0.019*\"complain\" + 0.018*\"pocket\"\n",
      "Topic: 68 Word: 0.024*\"polit\" + 0.023*\"invent\" + 0.017*\"funni\" + 0.016*\"promis\" + 0.016*\"politician\" + 0.015*\"park\" + 0.015*\"feedback\" + 0.013*\"copi\" + 0.013*\"trust\" + 0.013*\"construct\"\n",
      "Topic: 69 Word: 0.025*\"messag\" + 0.025*\"news\" + 0.013*\"match\" + 0.011*\"unit\" + 0.011*\"win\" + 0.010*\"teach\" + 0.010*\"griefer\" + 0.009*\"hang\" + 0.008*\"aswel\" + 0.008*\"winner\"\n",
      "Topic: 70 Word: 0.048*\"chair\" + 0.045*\"hell\" + 0.040*\"fulli\" + 0.036*\"fanfest\" + 0.020*\"constitu\" + 0.019*\"spot\" + 0.018*\"organis\" + 0.017*\"pictur\" + 0.017*\"snip\" + 0.017*\"weren\"\n",
      "Topic: 71 Word: 0.097*\"goon\" + 0.022*\"nerf\" + 0.022*\"nano\" + 0.018*\"csms\" + 0.012*\"girl\" + 0.012*\"impress\" + 0.010*\"conflict\" + 0.008*\"game\" + 0.008*\"boot\" + 0.007*\"post\"\n",
      "Topic: 72 Word: 0.024*\"ballot\" + 0.019*\"vote\" + 0.016*\"cast\" + 0.011*\"logo\" + 0.011*\"boil\" + 0.010*\"consult\" + 0.008*\"volunt\" + 0.008*\"player\" + 0.008*\"game\" + 0.008*\"deni\"\n",
      "Topic: 73 Word: 0.041*\"definit\" + 0.033*\"tank\" + 0.033*\"shield\" + 0.022*\"boost\" + 0.021*\"slot\" + 0.021*\"bonus\" + 0.019*\"armor\" + 0.019*\"brain\" + 0.016*\"ship\" + 0.014*\"resist\"\n",
      "Topic: 74 Word: 0.087*\"scam\" + 0.033*\"save\" + 0.028*\"wish\" + 0.027*\"roleplay\" + 0.025*\"worst\" + 0.020*\"roll\" + 0.019*\"nomin\" + 0.019*\"letter\" + 0.018*\"wanna\" + 0.018*\"appropri\"\n",
      "Topic: 75 Word: 0.266*\"vote\" + 0.052*\"servic\" + 0.047*\"product\" + 0.024*\"platform\" + 0.022*\"count\" + 0.019*\"sens\" + 0.019*\"support\" + 0.013*\"make\" + 0.010*\"anybodi\" + 0.010*\"touch\"\n",
      "Topic: 76 Word: 0.052*\"shadow\" + 0.035*\"read\" + 0.032*\"shame\" + 0.031*\"prioriti\" + 0.027*\"exist\" + 0.025*\"choic\" + 0.021*\"knowledg\" + 0.019*\"list\" + 0.019*\"biomass\" + 0.019*\"pass\"\n",
      "Topic: 77 Word: 0.042*\"wormhol\" + 0.016*\"space\" + 0.015*\"site\" + 0.015*\"explor\" + 0.013*\"ankh\" + 0.013*\"poll\" + 0.011*\"live\" + 0.010*\"tag\" + 0.008*\"null\" + 0.008*\"insult\"\n",
      "Topic: 78 Word: 0.042*\"cool\" + 0.022*\"dev\" + 0.022*\"stori\" + 0.019*\"rage\" + 0.017*\"histori\" + 0.014*\"languag\" + 0.013*\"desper\" + 0.013*\"embarrass\" + 0.013*\"broadcast\" + 0.012*\"abandon\"\n",
      "Topic: 79 Word: 0.019*\"info\" + 0.018*\"secret\" + 0.017*\"human\" + 0.014*\"thousand\" + 0.014*\"team\" + 0.012*\"crew\" + 0.012*\"work\" + 0.012*\"nerd\" + 0.011*\"describ\" + 0.009*\"notif\"\n",
      "Topic: 80 Word: 0.036*\"implant\" + 0.034*\"clone\" + 0.017*\"pod\" + 0.014*\"citadel\" + 0.012*\"lose\" + 0.012*\"ownership\" + 0.011*\"afford\" + 0.011*\"cute\" + 0.011*\"wind\" + 0.010*\"lesson\"\n",
      "Topic: 81 Word: 0.030*\"scan\" + 0.022*\"probe\" + 0.019*\"scanner\" + 0.016*\"signatur\" + 0.012*\"sweet\" + 0.012*\"mine\" + 0.011*\"elit\" + 0.010*\"roid\" + 0.009*\"detect\" + 0.009*\"investig\"\n",
      "Topic: 82 Word: 0.073*\"wrong\" + 0.049*\"hilari\" + 0.036*\"char\" + 0.035*\"cours\" + 0.030*\"portion\" + 0.030*\"troubl\" + 0.024*\"split\" + 0.023*\"drag\" + 0.022*\"flame\" + 0.020*\"web\"\n",
      "Topic: 83 Word: 0.037*\"particip\" + 0.029*\"free\" + 0.026*\"figur\" + 0.024*\"kid\" + 0.023*\"opinion\" + 0.022*\"hilmar\" + 0.019*\"ticket\" + 0.019*\"kugu\" + 0.018*\"believ\" + 0.018*\"threat\"\n",
      "Topic: 84 Word: 0.016*\"heart\" + 0.015*\"defend\" + 0.012*\"null\" + 0.010*\"allianc\" + 0.010*\"daili\" + 0.009*\"attack\" + 0.009*\"worthi\" + 0.008*\"basi\" + 0.008*\"fight\" + 0.007*\"group\"\n",
      "Topic: 85 Word: 0.059*\"channel\" + 0.037*\"chat\" + 0.035*\"reform\" + 0.027*\"amus\" + 0.018*\"network\" + 0.017*\"hmmm\" + 0.016*\"revers\" + 0.016*\"bull\" + 0.015*\"pop\" + 0.015*\"memori\"\n",
      "Topic: 86 Word: 0.030*\"thank\" + 0.024*\"silent\" + 0.021*\"object\" + 0.020*\"speak\" + 0.018*\"major\" + 0.018*\"easi\" + 0.016*\"pubbi\" + 0.016*\"freighter\" + 0.008*\"ship\" + 0.007*\"logi\"\n",
      "Topic: 87 Word: 0.038*\"devil\" + 0.027*\"advoc\" + 0.026*\"status\" + 0.025*\"secur\" + 0.024*\"interest\" + 0.019*\"statement\" + 0.018*\"beat\" + 0.015*\"valid\" + 0.014*\"cancel\" + 0.013*\"hors\"\n",
      "Topic: 88 Word: 0.031*\"name\" + 0.026*\"incurs\" + 0.019*\"speech\" + 0.017*\"disband\" + 0.016*\"bitter\" + 0.016*\"rifter\" + 0.015*\"hop\" + 0.015*\"bomber\" + 0.015*\"stealth\" + 0.015*\"everybodi\"\n",
      "Topic: 89 Word: 0.019*\"password\" + 0.018*\"rank\" + 0.016*\"leadership\" + 0.014*\"googl\" + 0.014*\"softwar\" + 0.012*\"march\" + 0.011*\"smoke\" + 0.011*\"permit\" + 0.011*\"uniqu\" + 0.010*\"browser\"\n",
      "Topic: 90 Word: 0.043*\"love\" + 0.033*\"internet\" + 0.030*\"spaceship\" + 0.026*\"appreci\" + 0.022*\"educ\" + 0.020*\"declar\" + 0.017*\"express\" + 0.017*\"horribl\" + 0.016*\"fellow\" + 0.014*\"entertain\"\n",
      "Topic: 91 Word: 0.088*\"ban\" + 0.042*\"crash\" + 0.029*\"tax\" + 0.027*\"brilliant\" + 0.027*\"walk\" + 0.025*\"alli\" + 0.023*\"percentag\" + 0.022*\"sand\" + 0.019*\"wise\" + 0.018*\"trap\"\n",
      "Topic: 92 Word: 0.094*\"elect\" + 0.031*\"hall\" + 0.029*\"assembl\" + 0.019*\"apolog\" + 0.018*\"nope\" + 0.017*\"orca\" + 0.015*\"militia\" + 0.015*\"children\" + 0.013*\"aggress\" + 0.013*\"jesus\"\n",
      "Topic: 93 Word: 0.055*\"page\" + 0.033*\"parti\" + 0.026*\"abus\" + 0.025*\"solv\" + 0.022*\"recruit\" + 0.022*\"problem\" + 0.022*\"influenc\" + 0.019*\"captain\" + 0.017*\"media\" + 0.015*\"quarter\"\n",
      "Topic: 94 Word: 0.033*\"dude\" + 0.023*\"self\" + 0.019*\"destruct\" + 0.014*\"crazi\" + 0.013*\"sisi\" + 0.012*\"stupid\" + 0.011*\"ration\" + 0.010*\"fals\" + 0.009*\"wallet\" + 0.008*\"tast\"\n",
      "Topic: 95 Word: 0.193*\"bump\" + 0.068*\"center\" + 0.048*\"video\" + 0.036*\"refer\" + 0.029*\"friend\" + 0.028*\"articl\" + 0.025*\"vile\" + 0.023*\"poetic\" + 0.022*\"respond\" + 0.018*\"butthurt\"\n",
      "Topic: 96 Word: 0.069*\"quot\" + 0.020*\"cri\" + 0.017*\"subject\" + 0.016*\"weekend\" + 0.015*\"confrimign\" + 0.014*\"wear\" + 0.014*\"blow\" + 0.012*\"reveal\" + 0.012*\"outcom\" + 0.012*\"sum\"\n",
      "Topic: 97 Word: 0.148*\"troll\" + 0.066*\"word\" + 0.037*\"queue\" + 0.032*\"drama\" + 0.026*\"event\" + 0.025*\"respons\" + 0.024*\"sugar\" + 0.022*\"right\" + 0.019*\"govern\" + 0.018*\"logic\"\n",
      "Topic: 98 Word: 0.079*\"alt\" + 0.036*\"runner\" + 0.028*\"bore\" + 0.018*\"mission\" + 0.014*\"cloth\" + 0.013*\"compon\" + 0.013*\"scheme\" + 0.013*\"ride\" + 0.012*\"manual\" + 0.012*\"dominion\"\n",
      "Topic: 99 Word: 0.082*\"hate\" + 0.070*\"block\" + 0.054*\"websit\" + 0.045*\"convinc\" + 0.041*\"reason\" + 0.031*\"plenti\" + 0.029*\"begin\" + 0.026*\"power\" + 0.023*\"mode\" + 0.022*\"outlin\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/gensim/models/ldamodel.py:1023: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=100, id2word=dictionary, passes=4, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't seem to be super useful in itself. It might be more useful to map it back to the comments and look for the most \"representative\" comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so we don't lose sight of all the summit topic threads, here's an index with a short summary about each one.team bff & the thousand little things.:in this thread, we welcome the community to offer suggestions about how csm can help team bff focus their efforts to get even more \"bang for buck\". we are not just looking for specific little things that annoy you and that you'd like fixed, but also suggestions about improving the process of identifying and prioritizing them.ship balance:in this thread, we're looking for everything relating to ship balancing, encompassing everything from individual ships to how they interact in fleets. the sky is the limit.nullsec industrialization (\"farms and fields\"):in this thread, we're looking for advice and feedback on how to improve nullsec industrialization. making nullsec less dependent on jita for everything, providing farms and fields for small-gang targets beyond sov structures. this may degenerate into a 'for gods sakes implement the missing dominion features' discussion.customer support:in this thread, we're looking for feedback on ways to improve eve customer support as a whole. it should focus on policies and procedures, whether new or existing, rather than individual petition details.new player experience:in this thread, we'll be discussing the eve npe. ccp has made great strides in improving the overall npe. we would like to gather feedback from current players on ways to improve the npe and ideas to increase player retention (by giving a good npe)ui improvements:in this thread, we're looking for methods to improve the eve ui. feel free to include links to illustrations of your ideas if you're a photoshop wizard.eve marketing:ccp zinfandel will be talking to the csm for an hour about ccp's marketing efforts related to eve. what advice would you want us to give him about seducing unsuspecting gamers into paying for the privilege of being podded? and what honey could he set out to lure back the elusive bittervet?\n",
      "['lose', 'sight', 'summit', 'topic', 'thread', 'index', 'short', 'summari', 'team', 'thousand', 'littl', 'thing', 'thread', 'welcom', 'communiti', 'offer', 'suggest', 'help', 'team', 'focus', 'effort', 'bang', 'buck', 'look', 'specif', 'littl', 'thing', 'annoy', 'like', 'fix', 'suggest', 'improv', 'process', 'identifi', 'priorit', 'ship', 'balanc', 'thread', 'look', 'relat', 'ship', 'balanc', 'encompass', 'individu', 'ship', 'interact', 'fleet', 'limit', 'nullsec', 'farm', 'field', 'thread', 'look', 'advic', 'feedback', 'improv', 'nullsec', 'make', 'nullsec', 'depend', 'jita', 'provid', 'farm', 'field', 'small', 'gang', 'target', 'structur', 'degener', 'god', 'sake', 'implement', 'miss', 'dominion', 'featur', 'discuss', 'custom', 'support', 'thread', 'look', 'feedback', 'way', 'improv', 'custom', 'support', 'focus', 'polici', 'procedur', 'exist', 'individu', 'petit', 'detail', 'player', 'experi', 'thread', 'discuss', 'great', 'stride', 'improv', 'overal', 'like', 'gather', 'feedback', 'current', 'player', 'way', 'improv', 'idea', 'increas', 'player', 'retent', 'give', 'good', 'improv', 'thread', 'look', 'method', 'improv', 'feel', 'free', 'includ', 'link', 'illustr', 'idea', 'photoshop', 'wizard', 'market', 'zinfandel', 'talk', 'hour', 'market', 'effort', 'relat', 'advic', 'want', 'seduc', 'unsuspect', 'gamer', 'pay', 'privileg', 'pod', 'honey', 'lure', 'elus', 'bittervet']\n",
      "\n",
      "what happened to the post of the long list of items the player base already voted on to be fixed from the last csm?\n",
      "['happen', 'post', 'long', 'list', 'item', 'player', 'base', 'vote', 'fix']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(documents[0])\n",
    "print(processed_docs[0])\n",
    "print()\n",
    "print(documents.loc[1])\n",
    "print(processed_docs[1])\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s\n",
      "['lose', 'sight', 'summit', 'topic', 'thread', 'index', 'short', 'summari', 'team', 'thousand', 'littl', 'thing', 'thread', 'welcom', 'communiti', 'offer', 'suggest', 'help', 'team', 'focus', 'effort', 'bang', 'buck', 'look', 'specif', 'littl', 'thing', 'annoy', 'like', 'fix', 'suggest', 'improv', 'process', 'identifi', 'priorit', 'ship', 'balanc', 'thread', 'look', 'relat', 'ship', 'balanc', 'encompass', 'individu', 'ship', 'interact', 'fleet', 'limit', 'nullsec', 'farm', 'field', 'thread', 'look', 'advic', 'feedback', 'improv', 'nullsec', 'make', 'nullsec', 'depend', 'jita', 'provid', 'farm', 'field', 'small', 'gang', 'target', 'structur', 'degener', 'god', 'sake', 'implement', 'miss', 'dominion', 'featur', 'discuss', 'custom', 'support', 'thread', 'look', 'feedback', 'way', 'improv', 'custom', 'support', 'focus', 'polici', 'procedur', 'exist', 'individu', 'petit', 'detail', 'player', 'experi', 'thread', 'discuss', 'great', 'stride', 'improv', 'overal', 'like', 'gather', 'feedback', 'current', 'player', 'way', 'improv', 'idea', 'increas', 'player', 'retent', 'give', 'good', 'improv', 'thread', 'look', 'method', 'improv', 'feel', 'free', 'includ', 'link', 'illustr', 'idea', 'photoshop', 'wizard', 'market', 'zinfandel', 'talk', 'hour', 'market', 'effort', 'relat', 'advic', 'want', 'seduc', 'unsuspect', 'gamer', 'pay', 'privileg', 'pod', 'honey', 'lure', 'elus', 'bittervet']\n",
      "\n",
      "Score: 0.8005160689353943\t \n",
      "Topic: 0.016*\"angri\" + 0.013*\"evid\" + 0.012*\"polici\" + 0.011*\"summari\" + 0.009*\"director\" + 0.009*\"rant\" + 0.009*\"peopl\" + 0.009*\"expans\" + 0.009*\"game\" + 0.009*\"univers\"\n",
      "\n",
      "Score: 0.08220750838518143\t \n",
      "Topic: 0.117*\"good\" + 0.098*\"like\" + 0.091*\"idea\" + 0.055*\"forget\" + 0.047*\"mouth\" + 0.046*\"seat\" + 0.029*\"english\" + 0.027*\"thread\" + 0.023*\"look\" + 0.020*\"stuff\"\n",
      "\n",
      "Score: 0.04307479411363602\t \n",
      "Topic: 0.034*\"corp\" + 0.031*\"wardec\" + 0.021*\"approv\" + 0.016*\"corpor\" + 0.012*\"player\" + 0.010*\"highsec\" + 0.008*\"join\" + 0.008*\"peopl\" + 0.007*\"asset\" + 0.007*\"pvper\"\n"
     ]
    }
   ],
   "source": [
    "print(documents.loc[0][0])\n",
    "print(processed_docs[0])\n",
    "for index, score in sorted(lda_model_tfidf[bow_corpus[0]], key=lambda tup: -1*tup[1])[:3]:\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31866"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "round(len(bow_corpus)*.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # very slow. Randomly select documents to label.\n",
    "# sample_fraction = 0.001 # sample as a percent of all posts\n",
    "# sample_size = round(len(bow_corpus)*sample_fraction)\n",
    "\n",
    "\n",
    "# # n_topics = 100\n",
    "\n",
    "\n",
    "# # map the model to posts\n",
    "# a = lda_model_tfidf.get_document_topics(bow_corpus[0],minimum_probability=0)\n",
    "\n",
    "# docs_LDA_scores = pd.DataFrame(columns = range(n_topics))\n",
    "\n",
    "# d = dict(a)\n",
    "# i=0\n",
    "# start = datetime.datetime.now()\n",
    "# for i in random.sample(range(len(bow_corpus)),sample_size):\n",
    "#     doc = bow_corpus[i]\n",
    "#     topics_of_doc = lda_model_tfidf.get_document_topics(doc,minimum_probability=0.00)\n",
    "#     v = np.array(topics_of_doc)[:,1]\n",
    "#     row = pd.DataFrame(v).transpose()\n",
    "#     docs_LDA_scores = docs_LDA_scores.append(row)\n",
    "    \n",
    "#     if i%1000 == 0:\n",
    "#         print(str(i)+\" posts   ||   time = \"+str(datetime.datetime.now()-start))\n",
    "#         docs_LDA_scores.to_csv('docs_LDA_scores.csv')\n",
    "#     i = i + 1\n",
    "# print(\"LDA Mapping - Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# i = random.randint(0,31000)\n",
    "# print( bow_corpus[i])\n",
    "# print(documents[i])\n",
    "# print(dataset.loc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# docs_LDA_scores.index = range(len(docs_LDA_scores))\n",
    "\n",
    "# show_n_comments = 20\n",
    "\n",
    "\n",
    "# for topic_num in range(n_topics):\n",
    "# # for topic_num in range(3,12):\n",
    "#     top_docs_idx = docs_LDA_scores.sort_values(topic_num,ascending=False)[topic_num][:show_n_comments].index\n",
    "#     print(\"Topic: \"+str(topic_num))\n",
    "#     print(lda_model_tfidf.print_topic(topic_num, 15))\n",
    "#     print()\n",
    "#     print('Comments:')\n",
    "#     for idx in top_docs_idx:\n",
    "#         print(\"[] \" + documents.loc[idx][0])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, LDA doesn't seem to be super useful. It might be more useful with entire threads, not just individual comments. Let's look at PCA / lexical networks next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at PCA \n",
    "Just cluster / PCA by TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA #, FastICA\n",
    "\n",
    "# corpus_tfidf_df = pd.DataFrame(corpus_tfidf)\n",
    "corpus_tfidf_csr = gensim.matutils.corpus2csc(corpus_tfidf)\n",
    "corpus_tfidf_numpy = corpus_tfidf_csr.T.toarray()\n",
    "\n",
    "# corpus_tfidf_array = corpus_tfidf.T.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca_tool = PCA(n_components=200)\n",
    "pca_tool.fit(corpus_tfidf_numpy)\n",
    "\n",
    "# ICA was too slow.\n",
    "# ica = FastICA(n_components=200)\n",
    "# ica.fit(corpus_tfidf_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "transformed_corpus = pca_tool.transform(corpus_tfidf_numpy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# rng = np.random.RandomState(1)\n",
    "X = transformed_corpus[:,[0,1]]\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.title('Comments projected into top 2 dimensions');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)\n",
    "plt.axis('equal');\n",
    "plt.title('Comments projected into top 2 dimensions');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca_tool.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prin_comps = pd.DataFrame(pca_tool.components_)\n",
    "prin_comps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for j in range(5):\n",
    "    \n",
    "    print(\"Component numer: \" + str(j))\n",
    "\n",
    "    component_j = prin_comps[j]\n",
    "\n",
    "    order = component_j.map(lambda x : x).abs().sort_values(ascending = False)\n",
    "    component_j[order.index]\n",
    "\n",
    "\n",
    "    for i in range(10):\n",
    "        idx = order.index[i]\n",
    "        word = dictionary[idx]\n",
    "        weight = str(component_j[idx])\n",
    "        print(weight + \" : \" + word)\n",
    "        \n",
    "    print()\n",
    "\n",
    "# sns.distplot(S);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These don't seem to be super interpretable. Try selecting representative comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformed_corpus_df = pd.DataFrame(transformed_corpus)\n",
    "\n",
    "\n",
    "# for the top component, select the indexes that are most associated with this comp and read them\n",
    "for comp_num in range(100):\n",
    "# comp_num = 0\n",
    "    print(\"Component numer: \" + str(comp_num))\n",
    "    sorted_df = transformed_corpus_df.sort_values(by=comp_num,ascending=False)\n",
    "    for top_comment_num in range(10):\n",
    "    #     top_comment_num = 0\n",
    "        top_comment_idx = sorted_df.index[top_comment_num]\n",
    "        print(documents.loc[top_comment_idx][0])\n",
    "        print()\n",
    "# order = component_j.map(lambda x : x).abs().sort_values(ascending = False)\n",
    "\n",
    "# component_j[order.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These seem to be more interpretable. The components seem to make more sense, although in some cases it is clear they are leaning heavily on certain words. \n",
    "\n",
    "Now let's go back to the network to see what emerges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's do a quick regression to see which components align with negativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "X = transformed_corpus\n",
    "y = list(documents['negative'])\n",
    "\n",
    "results = sm.OLS(y, X).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go back to the network to see what emerges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "#### Hierarchical Clustering\n",
    "PCA already reduced the dimensionality, so Hierarchical Clustering should not be too too slow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical Networks\n",
    "Plan:\n",
    "1. Add bigrams and trigrams to the dictionary (can do later)\n",
    "    2. Then filter it again for low-frequency terms or super high-freq terms (>50%)\n",
    "3. Create an M x M matrix, where M is the number of terms in the dictionary\n",
    "4. O(N) to fill out the matrix. Iterate through the list of comments (That have been reduced to bag of words/terms). In each post, for each pair of words, increment the value in the matrix, weighted by TFIDF (the product of the two terms' value).\n",
    "5. Once done, make the matrix sparser by thresholding values below the 50% (median) value.\n",
    "6. Visualize the matrix (probably get a hairball). Can tune the thresholding until getting cleaner separation.\n",
    "7. Run community detection algo to get 'themes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# don't even need to create a matrix (too much space). Just create a dictionary (link_list)\n",
    "idx = 0\n",
    "link_weights = {}\n",
    "link_text = {}\n",
    "for post in corpus_tfidf:\n",
    "    L = len(post)\n",
    "    # cycle thru each word (i) in the post and the subsequent words (j).\n",
    "    # if this pair of words is unseen, add it to the dict of weights. Otherwise, add it to the existing entry\n",
    "    for i in range(L):\n",
    "        for j in range(i,L):\n",
    "            if (i<j):\n",
    "                weight = post[i][1]*post[j][1]\n",
    "#                 print(str(i) + \" , \" + str(j) + \": \" + str(weight))\n",
    "                \n",
    "                the_key = (dictionary[post[i][0]],dictionary[post[j][0]])\n",
    "                if the_key in link_weights:\n",
    "                    link_weights[the_key] = link_weights[the_key]+ weight\n",
    "#                     link_text[the_key].append(documents.loc[idx][0])\n",
    "                else:\n",
    "                    link_weights[the_key] = weight\n",
    "                    link_text[the_key] = [documents.loc[idx][0]]\n",
    "    if idx % 500 == 0:\n",
    "        print(str(idx)+\" posts   ||   date-time = \"+str(datetime.datetime.now()))\n",
    "    idx = idx + 1\n",
    "print('Done.')\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# link_weights.keys()\n",
    "# link_weights.values()\n",
    "k = list(link_weights.keys())\n",
    "v = list(link_weights.values())\n",
    "d = {'links': k, 'weights': v}\n",
    "\n",
    "link_weights_df = pd.DataFrame(d)\n",
    "link_weights_df.sort_values(by='weights',ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(link_weights_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"strongest\" link is (item, vaniti)\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "#Log Scale\n",
    "sns.distplot(np.log(v));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only the links with strong weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "order = link_weights_df.sort_values(by='weights',ascending=False).index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many links? top 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# top_pct = 0.10\n",
    "# top_N = top_pct*len(order)\n",
    "top_N = 100# just pick 100 arbitrarily\n",
    "top_links = order[:top_N]\n",
    "top_link_weights_df = link_weights_df.loc[top_links]\n",
    "# top_link_weights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# need to split the links into separate columns after all\n",
    "\n",
    "top_link_weights_df[['node_i','node_j']] = top_link_weights_df['links'].apply(pd.Series)\n",
    "top_link_weights_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a Graph \n",
    "G=nx.from_pandas_dataframe(top_link_weights_df, 'node_i', 'node_j', ['weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_spectral(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spectral representation looks like it structures the nodes well. Maybe try spectral clustering on the network to find the key themes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn import metrics\n",
    "\n",
    "n_clusters = 8\n",
    "\n",
    "# Get adjacency-matrix as numpy-array\n",
    "adj_mat = nx.to_numpy_matrix(G,weight='weights')\n",
    "\n",
    "# Cluster\n",
    "sc = SpectralClustering(n_clusters, affinity='precomputed', n_init=100)\n",
    "sc.fit(adj_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mat.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(sc.labels_,normed=True,bins=n_clusters); plt.title('spectral clustering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try another clustering algo\n",
    "# from networkx.algorithms.community import greedy_modularity_communities\n",
    "# import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first compute the best partition\n",
    "# partition = community.best_partition(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pagerank = nx.eigenvector_centrality_numpy(G)\n",
    "# k_pr = list(pagerank.keys())\n",
    "# v_pr = list(pagerank.values())\n",
    "# df_pr = pd.DataFrame({'term' : k_pr, 'eigen_centrality':v_pr})\n",
    "# pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'term' : G.nodes(), 'cluster' : sc.labels_})\n",
    "df['eigen_centrality'] = 0\n",
    "for idx in df.index:\n",
    "    term = df.loc[idx,'term']\n",
    "    df.loc[idx,'eigen_centrality'] = pagerank[term]\n",
    "df.sort_values('eigen_centrality',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create-subgraphs and visualize each one.\n",
    "top_link_weights_df[['node_i','node_j']]\n",
    "\n",
    "# df.sort_values(by='cluster')\n",
    "\n",
    "sub_community = {}\n",
    "for clust_id in range(n_clusters):\n",
    "    community_nodes = list(df[df['cluster']==clust_id]['term'])\n",
    "    sub_community[clust_id] = top_link_weights_df[top_link_weights_df['node_i'].isin(community_nodes)]\n",
    "    print(df[df['cluster']==clust_id].sort_values('eigen_centrality',ascending=False))\n",
    "    print()\n",
    "\n",
    "# for each term, get the top weighted link\n",
    "# for each link, get a comment with that link in it (this might take more time, should do ahead of time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(3,figsize=(12,12)) \n",
    "nx.draw(G,with_labels=True,node_size=4000*df['eigen_centrality'],node_color=sc.labels_,font_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clust_id = 1\n",
    "sub_G=nx.from_pandas_dataframe(sub_community[clust_id], 'node_i', 'node_j', ['weights'])\n",
    "sub_community[clust_id]\n",
    "community_nodes = list(df[df['cluster']==clust_id]['term'])\n",
    "in_commnity =  pd.Series(sub_G.nodes()).isin(community_nodes)\n",
    "\n",
    "df2 = pd.DataFrame({'term' : sub_G.nodes()})\n",
    "df2['eigen_centrality'] = 0\n",
    "for idx in df2.index:\n",
    "    term = df2.loc[idx,'term']\n",
    "    df2.loc[idx,'eigen_centrality'] = pagerank[term]\n",
    "# df2.sort_values('eigen_centrality',ascending=False)\n",
    "# larger figure size\n",
    "plt.figure(3,figsize=(12,12)) \n",
    "nx.draw(sub_G,with_labels=True,node_size=10*in_commnity,node_color=in_commnity*2,cmap=plt.get_cmap('rainbow'),font_size=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# larger figure size\n",
    "plt.figure(3,figsize=(12,12)) \n",
    "nx.draw(sub_G,with_labels=True,node_size=10*in_commnity,node_color=in_commnity*2,cmap=plt.get_cmap('rainbow'),font_size=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many links? top 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# top_pct = 0.10\n",
    "# top_N = top_pct*len(order)\n",
    "top_N = 1000# just pick 1000 arbitrarily\n",
    "top_links = order[:top_N]\n",
    "top_link_weights_df = link_weights_df.loc[top_links]\n",
    "# top_link_weights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# need to split the links into separate columns after all\n",
    "\n",
    "top_link_weights_df[['node_i','node_j']] = top_link_weights_df['links'].apply(pd.Series)\n",
    "top_link_weights_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a Graph \n",
    "G=nx.from_pandas_dataframe(top_link_weights_df, 'node_i', 'node_j', ['weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nx.draw_spectral(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spectral representation looks like it structures the nodes well. Maybe try spectral clustering on the network to find the key themes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn import metrics\n",
    "\n",
    "n_clusters = 50\n",
    "\n",
    "# Get adjacency-matrix as numpy-array\n",
    "adj_mat = nx.to_numpy_matrix(G,weight='weights')\n",
    "\n",
    "# Cluster\n",
    "sc = SpectralClustering(n_clusters, affinity='precomputed', n_init=100)\n",
    "sc.fit(adj_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mat.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.hist(sc.labels_,normed=True,bins=n_clusters); plt.title('spectral clustering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try another clustering algo\n",
    "# from networkx.algorithms.community import greedy_modularity_communities\n",
    "# import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first compute the best partition\n",
    "# partition = community.best_partition(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pagerank = nx.eigenvector_centrality_numpy(G)\n",
    "# k_pr = list(pagerank.keys())\n",
    "# v_pr = list(pagerank.values())\n",
    "# df_pr = pd.DataFrame({'term' : k_pr, 'eigen_centrality':v_pr})\n",
    "# pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'term' : G.nodes(), 'cluster' : sc.labels_})\n",
    "df['eigen_centrality'] = 0\n",
    "for idx in df.index:\n",
    "    term = df.loc[idx,'term']\n",
    "    df.loc[idx,'eigen_centrality'] = pagerank[term]\n",
    "df.sort_values('eigen_centrality',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create-subgraphs and visualize each one.\n",
    "top_link_weights_df[['node_i','node_j']]\n",
    "\n",
    "# df.sort_values(by='cluster')\n",
    "\n",
    "sub_community = {}\n",
    "for clust_id in range(n_clusters):\n",
    "    community_nodes = list(df[df['cluster']==clust_id]['term'])\n",
    "    sub_community[clust_id] = top_link_weights_df[top_link_weights_df['node_i'].isin(community_nodes)]\n",
    "    print(df[df['cluster']==clust_id].sort_values('eigen_centrality',ascending=False))\n",
    "    print()\n",
    "\n",
    "# for each term, get the top weighted link\n",
    "# for each link, get a comment with that link in it (this might take more time, should do ahead of time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(3,figsize=(12,12)) \n",
    "nx.draw(G,with_labels=True,node_size=4000*df['eigen_centrality'],node_color=sc.labels_,font_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clust_id = 17\n",
    "sub_G=nx.from_pandas_dataframe(sub_community[clust_id], 'node_i', 'node_j', ['weights'])\n",
    "sub_community[clust_id]\n",
    "community_nodes = list(df[df['cluster']==clust_id]['term'])\n",
    "in_commnity =  pd.Series(sub_G.nodes()).isin(community_nodes)\n",
    "\n",
    "df2 = pd.DataFrame({'term' : sub_G.nodes()})\n",
    "df2['eigen_centrality'] = 0\n",
    "for idx in df2.index:\n",
    "    term = df2.loc[idx,'term']\n",
    "    df2.loc[idx,'eigen_centrality'] = pagerank[term]\n",
    "# df2.sort_values('eigen_centrality',ascending=False)\n",
    "# larger figure size\n",
    "plt.figure(3,figsize=(12,12)) \n",
    "nx.draw(sub_G,with_labels=True,node_size=1500*in_commnity,node_color=in_commnity*2,cmap=plt.get_cmap('rainbow'),font_size=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get sub-clusters from cluster 0\n",
    "clust_id = 0\n",
    "sub_G=nx.from_pandas_dataframe(sub_community[clust_id], 'node_i', 'node_j', ['weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# larger figure size\n",
    "plt.figure(3,figsize=(12,12)) \n",
    "nx.draw(sub_G,with_labels=True,node_size=10*in_commnity,node_color=in_commnity*2,cmap=plt.get_cmap('rainbow'),font_size=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(3,figsize=(12,12)) \n",
    "nx.draw(G,with_labels=True,node_size=1000*df['eigen_centrality'],node_color=sc.labels_,font_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many links? top 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# top_pct = 0.10\n",
    "# top_N = top_pct*len(order)\n",
    "top_N = 600# just pick 1000 arbitrarily\n",
    "top_links = order[:top_N]\n",
    "top_link_weights_df = link_weights_df.loc[top_links]\n",
    "# top_link_weights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# need to split the links into separate columns after all\n",
    "\n",
    "top_link_weights_df[['node_i','node_j']] = top_link_weights_df['links'].apply(pd.Series)\n",
    "top_link_weights_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a Graph \n",
    "G=nx.from_pandas_dataframe(top_link_weights_df, 'node_i', 'node_j', ['weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_spectral(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spectral representation looks like it structures the nodes well. Maybe try spectral clustering on the network to find the key themes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn import metrics\n",
    "\n",
    "n_clusters = 8\n",
    "\n",
    "# Get adjacency-matrix as numpy-array\n",
    "adj_mat = nx.to_numpy_matrix(G,weight='weights')\n",
    "\n",
    "# Cluster\n",
    "sc = SpectralClustering(n_clusters, affinity='precomputed', n_init=100)\n",
    "sc.fit(adj_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mat.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(sc.labels_,normed=True,bins=n_clusters); plt.title('spectral clustering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try another clustering algo\n",
    "# from networkx.algorithms.community import greedy_modularity_communities\n",
    "# import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first compute the best partition\n",
    "# partition = community.best_partition(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pagerank = nx.eigenvector_centrality_numpy(G)\n",
    "# k_pr = list(pagerank.keys())\n",
    "# v_pr = list(pagerank.values())\n",
    "# df_pr = pd.DataFrame({'term' : k_pr, 'eigen_centrality':v_pr})\n",
    "# pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'term' : G.nodes(), 'cluster' : sc.labels_})\n",
    "df['eigen_centrality'] = 0\n",
    "for idx in df.index:\n",
    "    term = df.loc[idx,'term']\n",
    "    df.loc[idx,'eigen_centrality'] = pagerank[term]\n",
    "df.sort_values('eigen_centrality',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create-subgraphs and visualize each one.\n",
    "top_link_weights_df[['node_i','node_j']]\n",
    "\n",
    "# df.sort_values(by='cluster')\n",
    "\n",
    "sub_community = {}\n",
    "for clust_id in range(n_clusters):\n",
    "    community_nodes = list(df[df['cluster']==clust_id]['term'])\n",
    "    sub_community[clust_id] = top_link_weights_df[top_link_weights_df['node_i'].isin(community_nodes)]\n",
    "    print(df[df['cluster']==clust_id].sort_values('eigen_centrality',ascending=False))\n",
    "    print()\n",
    "\n",
    "# for each term, get the top weighted link\n",
    "# for each link, get a comment with that link in it (this might take more time, should do ahead of time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clust_id = 1\n",
    "sub_G=nx.from_pandas_dataframe(sub_community[clust_id], 'node_i', 'node_j', ['weights'])\n",
    "sub_community[clust_id]\n",
    "community_nodes = list(df[df['cluster']==clust_id]['term'])\n",
    "in_commnity =  pd.Series(sub_G.nodes()).isin(community_nodes)\n",
    "\n",
    "df2 = pd.DataFrame({'term' : sub_G.nodes()})\n",
    "df2['eigen_centrality'] = 0\n",
    "for idx in df2.index:\n",
    "    term = df2.loc[idx,'term']\n",
    "    df2.loc[idx,'eigen_centrality'] = pagerank[term]\n",
    "# df2.sort_values('eigen_centrality',ascending=False)\n",
    "# larger figure size\n",
    "plt.figure(3,figsize=(12,12)) \n",
    "nx.draw(sub_G,with_labels=True,node_size=10*in_commnity,node_color=in_commnity*2,cmap=plt.get_cmap('rainbow'),font_size=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# larger figure size\n",
    "plt.figure(3,figsize=(12,12)) \n",
    "nx.draw(sub_G,with_labels=True,node_size=10*in_commnity,node_color=in_commnity*2,cmap=plt.get_cmap('rainbow'),font_size=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(3,figsize=(12,12)) \n",
    "nx.draw(G,with_labels=True,node_size=1000*df['eigen_centrality'],node_color=sc.labels_,font_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_spring(G,with_labels=True,node_size=1000*df['eigen_centrality'],node_color=sc.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_spectral(G,node_size=100,node_color=sc.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given: list of nodes (N), and list of edges E=(u,v), get the list of edges that intersects with nodes in N\n",
    "top_link_weights_df[top_link_weights_df['node_i'].isin(community_nodes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "which = lambda lst:list(np.where(lst)[0])\n",
    "\n",
    "\n",
    "# community_nodes = ['transact', 'micro']\n",
    "# ni = top_link_weights_df['node_i']\n",
    "# nj = top_link_weights_df['node_j']\n",
    "# matches = [True for x in ni if x in community_nodes]\n",
    "# matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents.loc[idx][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "1. Look more at the forumns / stuff qualitatively\n",
    "2. Develop a qualitative nomological network\n",
    "3. perhaps use some emergent constructs from the findings above (e.g. types of posts, themes, etc.) but be very open to seeing new constructs that cannot be seen directly from the vocab (e.g. power, old-guard, crowd, etc.)\n",
    "4. Think about how to integrate emerging ideas & constructs with this text analysis. \n",
    "    1. Filtering (e.g. by key-word)\n",
    "    2. By Negativity?\n",
    "    3. Modeling? PCA - negativity?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
